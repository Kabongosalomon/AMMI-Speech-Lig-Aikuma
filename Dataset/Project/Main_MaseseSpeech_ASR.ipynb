{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch==1.5.1 in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.5.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.18.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.5.1->torchvision) (0.18.2)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.6.1\n",
      "Requirement already satisfied: ipdb in /opt/conda/lib/python3.7/site-packages (0.13.3)\n",
      "Requirement already satisfied: ipython>=5.1.0; python_version >= \"3.4\" in /opt/conda/lib/python3.7/site-packages (from ipdb) (7.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from ipdb) (46.2.0.post20200511)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.3.3)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.15.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (3.0.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.5)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.14.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.5.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.1.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.6.0)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (0.5.1)\n",
      "Requirement already satisfied: torch==1.5.1 in /opt/conda/lib/python3.7/site-packages (from torchaudio) (1.5.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.5.1->torchaudio) (0.18.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.5.1->torchaudio) (1.18.4)\n",
      "Processing /home/jupyter/.cache/pip/wheels/57/cc/07/6aac75f5395a224650905accd38c868c2276782a56f1046b7b/PyDrive-1.3.1-py3-none-any.whl\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (4.1.3)\n",
      "Requirement already satisfied: google-api-python-client>=1.2 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (1.8.2)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /opt/conda/lib/python3.7/site-packages (from PyDrive) (5.3.1)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.17.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
      "Requirement already satisfied: six>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=4.0.0->PyDrive) (1.14.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.14.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (1.17.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (46.2.0.post20200511)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.2->PyDrive) (3.11.4)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.2->PyDrive) (2020.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client>=1.2->PyDrive) (1.51.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client>=1.2->PyDrive) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client>=1.2->PyDrive) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client>=1.2->PyDrive) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n",
      "Installing collected packages: PyDrive\n",
      "Successfully installed PyDrive-1.3.1\n",
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.7/site-packages (0.10.3.post1)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile) (2.18)\n"
     ]
    }
   ],
   "source": [
    "# If you need some install, uncomment the code bellow \n",
    "\n",
    "# !pip install torch==1.4\n",
    "!pip install torchvision\n",
    "!pip install ipdb\n",
    "\n",
    "!pip install torchaudio\n",
    "!pip install PyDrive\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import ipdb\n",
    "\n",
    "# from torchaudio.datasets import YESNO, LIBRISPEECH\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pdb, traceback, sys\n",
    "\n",
    "\n",
    "from torchaudio.datasets.utils import (\n",
    "  download_url,\n",
    "  extract_archive,\n",
    "  walk_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaseseSpeech 2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course tutorial Data\n",
    "\n",
    "# train-clean-100_small_subset_val.zip\n",
    "# https://drive.google.com/file/d/1SceZ97xNyHDev-UbnBtlJFn9RJ_Skg6I/view?usp=sharing # \n",
    "\n",
    "# train-clean-100_small_subset_train.zip \n",
    "# https://drive.google.com/uc?id=1SceZ97xNyHDev-UbnBtlJFn9RJ_Skg6I&export=download \n",
    "\n",
    "\n",
    "# converted_aligned_phones.zip\n",
    "# https://drive.google.com/file/d/1bLuDkapGBERG_VYPS7fNZl5GXsQ9z3p2/view\n",
    "\n",
    "\n",
    "# trainSeqs_1.0.zip\n",
    "# https://drive.google.com/file/d/1EnR-4zVXetYSYEMLGRWDSFhc0wmjIKmf/view\n",
    "\n",
    "\n",
    "# valSeqs_1.0.zip\n",
    "# https://drive.google.com/file/d/1gLOPrWa_-9f_F8aXMasKyrm61rs11Slu/view\n",
    "\n",
    "\n",
    "# validated_phones_reduced.txt\n",
    "# https://drive.google.com/file/d/19eScg1DijWzKHMQsHqCQm68aXt9lGE03/view\n",
    "\n",
    "# pack_master.zip\n",
    "# https://drive.google.com/file/d/1CFDTYEGxvoqTRWqHCh_4M1kEEtTekeyD/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train link Drive\n",
    "# https://drive.google.com/file/d/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR/view?usp=sharing\n",
    "    \n",
    "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\" -O train-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "    \n",
    "# # Valid link Drive\n",
    "# https://drive.google.com/file/d/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP/view?usp=sharing\n",
    "# https://drive.google.com/file/d/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP/view?usp=sharing\n",
    "    \n",
    "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\" -O dev-clean.tar.xz && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-05 17:20:39--  https://github.com/Kabongosalomon/MaseseSpeech/raw/master/voalingala.tar.xz\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Kabongosalomon/MaseseSpeech/master/voalingala.tar.xz [following]\n",
      "--2020-07-05 17:20:39--  https://raw.githubusercontent.com/Kabongosalomon/MaseseSpeech/master/voalingala.tar.xz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32168212 (31M) [application/octet-stream]\n",
      "Saving to: ‘voalingala.tar.xz’\n",
      "\n",
      "voalingala.tar.xz   100%[===================>]  30.68M  13.5MB/s    in 2.3s    \n",
      "\n",
      "2020-07-05 17:20:42 (13.5 MB/s) - ‘voalingala.tar.xz’ saved [32168212/32168212]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget  https://github.com/Kabongosalomon/MaseseSpeech/raw/master/voalingala.tar.xz -O voalingala.tar.xz\n",
    "\n",
    "extract_archive(\"voalingala.tar.xz\")\n",
    "!rm -r \"voalingala.tar.xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘MaseseSpeech’: File exists\n",
      "--2020-07-05 17:20:46--  https://docs.google.com/uc?export=download&confirm=aKYb&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\n",
      "Resolving docs.google.com (docs.google.com)... 74.125.195.138, 74.125.195.113, 74.125.195.102, ...\n",
      "Connecting to docs.google.com (docs.google.com)|74.125.195.138|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-04-64-docs.googleusercontent.com/docs/securesc/ilbrv8hfn7rtb44t5r4qeth979sfdkrc/jcml8uh3q6esminkbr0a1lubiidk21v4/1593969600000/00743790830645195323/07504609058036753451Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e=download [following]\n",
      "--2020-07-05 17:20:46--  https://doc-04-64-docs.googleusercontent.com/docs/securesc/ilbrv8hfn7rtb44t5r4qeth979sfdkrc/jcml8uh3q6esminkbr0a1lubiidk21v4/1593969600000/00743790830645195323/07504609058036753451Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e=download\n",
      "Resolving doc-04-64-docs.googleusercontent.com (doc-04-64-docs.googleusercontent.com)... 173.194.203.132, 2607:f8b0:400e:c05::84\n",
      "Connecting to doc-04-64-docs.googleusercontent.com (doc-04-64-docs.googleusercontent.com)|173.194.203.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=2j0e9mn3gvqiu&continue=https://doc-04-64-docs.googleusercontent.com/docs/securesc/ilbrv8hfn7rtb44t5r4qeth979sfdkrc/jcml8uh3q6esminkbr0a1lubiidk21v4/1593969600000/00743790830645195323/07504609058036753451Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e%3Ddownload&hash=vrh2raj237t1dlgcng74nvs4l7kfjesd [following]\n",
      "--2020-07-05 17:20:46--  https://docs.google.com/nonceSigner?nonce=2j0e9mn3gvqiu&continue=https://doc-04-64-docs.googleusercontent.com/docs/securesc/ilbrv8hfn7rtb44t5r4qeth979sfdkrc/jcml8uh3q6esminkbr0a1lubiidk21v4/1593969600000/00743790830645195323/07504609058036753451Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e%3Ddownload&hash=vrh2raj237t1dlgcng74nvs4l7kfjesd\n",
      "Connecting to docs.google.com (docs.google.com)|74.125.195.138|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-04-64-docs.googleusercontent.com/docs/securesc/ilbrv8hfn7rtb44t5r4qeth979sfdkrc/jcml8uh3q6esminkbr0a1lubiidk21v4/1593969600000/00743790830645195323/07504609058036753451Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e=download&nonce=2j0e9mn3gvqiu&user=07504609058036753451Z&hash=qrls8g0iln89lre04bh9isepkq81o4km [following]\n",
      "--2020-07-05 17:20:46--  https://doc-04-64-docs.googleusercontent.com/docs/securesc/ilbrv8hfn7rtb44t5r4qeth979sfdkrc/jcml8uh3q6esminkbr0a1lubiidk21v4/1593969600000/00743790830645195323/07504609058036753451Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e=download&nonce=2j0e9mn3gvqiu&user=07504609058036753451Z&hash=qrls8g0iln89lre04bh9isepkq81o4km\n",
      "Connecting to doc-04-64-docs.googleusercontent.com (doc-04-64-docs.googleusercontent.com)|173.194.203.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-xz]\n",
      "Saving to: ‘MaseseSpeech/train-clean.tar.xz’\n",
      "\n",
      "MaseseSpeech/train-     [       <=>          ] 124.63M  70.0MB/s    in 1.8s    \n",
      "\n",
      "2020-07-05 17:20:48 (70.0 MB/s) - ‘MaseseSpeech/train-clean.tar.xz’ saved [130680092]\n",
      "\n",
      "--2020-07-05 17:21:02--  https://docs.google.com/uc?export=download&confirm=&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\n",
      "Resolving docs.google.com (docs.google.com)... 74.125.197.139, 74.125.197.100, 74.125.197.113, ...\n",
      "Connecting to docs.google.com (docs.google.com)|74.125.197.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-14-1o-docs.googleusercontent.com/docs/securesc/f3cvssfpq8gf3a417blqk8g4is9dh7ng/ag5mbqupan2gapg6akv2vtig6p7rnbfv/1593969600000/00743790830645195323/15582857131208989249Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e=download [following]\n",
      "--2020-07-05 17:21:03--  https://doc-14-1o-docs.googleusercontent.com/docs/securesc/f3cvssfpq8gf3a417blqk8g4is9dh7ng/ag5mbqupan2gapg6akv2vtig6p7rnbfv/1593969600000/00743790830645195323/15582857131208989249Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e=download\n",
      "Resolving doc-14-1o-docs.googleusercontent.com (doc-14-1o-docs.googleusercontent.com)... 173.194.203.132, 2607:f8b0:400e:c05::84\n",
      "Connecting to doc-14-1o-docs.googleusercontent.com (doc-14-1o-docs.googleusercontent.com)|173.194.203.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=q1poga07e6tte&continue=https://doc-14-1o-docs.googleusercontent.com/docs/securesc/f3cvssfpq8gf3a417blqk8g4is9dh7ng/ag5mbqupan2gapg6akv2vtig6p7rnbfv/1593969600000/00743790830645195323/15582857131208989249Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e%3Ddownload&hash=hahtgb9vjtbls64iq9ptd9lckgvino7s [following]\n",
      "--2020-07-05 17:21:03--  https://docs.google.com/nonceSigner?nonce=q1poga07e6tte&continue=https://doc-14-1o-docs.googleusercontent.com/docs/securesc/f3cvssfpq8gf3a417blqk8g4is9dh7ng/ag5mbqupan2gapg6akv2vtig6p7rnbfv/1593969600000/00743790830645195323/15582857131208989249Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e%3Ddownload&hash=hahtgb9vjtbls64iq9ptd9lckgvino7s\n",
      "Connecting to docs.google.com (docs.google.com)|74.125.197.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-14-1o-docs.googleusercontent.com/docs/securesc/f3cvssfpq8gf3a417blqk8g4is9dh7ng/ag5mbqupan2gapg6akv2vtig6p7rnbfv/1593969600000/00743790830645195323/15582857131208989249Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e=download&nonce=q1poga07e6tte&user=15582857131208989249Z&hash=tv9ps8god3te9gufva94v8tgeakqpjqu [following]\n",
      "--2020-07-05 17:21:04--  https://doc-14-1o-docs.googleusercontent.com/docs/securesc/f3cvssfpq8gf3a417blqk8g4is9dh7ng/ag5mbqupan2gapg6akv2vtig6p7rnbfv/1593969600000/00743790830645195323/15582857131208989249Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e=download&nonce=q1poga07e6tte&user=15582857131208989249Z&hash=tv9ps8god3te9gufva94v8tgeakqpjqu\n",
      "Connecting to doc-14-1o-docs.googleusercontent.com (doc-14-1o-docs.googleusercontent.com)|173.194.203.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-xz]\n",
      "Saving to: ‘MaseseSpeech/dev-clean.tar.xz’\n",
      "\n",
      "MaseseSpeech/dev-cl     [    <=>             ]  89.37M   141MB/s    in 0.6s    \n",
      "\n",
      "2020-07-05 17:21:04 (141 MB/s) - ‘MaseseSpeech/dev-clean.tar.xz’ saved [93709260]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir MaseseSpeech\n",
    "!mkdir MaseseSpeech\n",
    "\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\" -O MaseseSpeech/train-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\" -O MaseseSpeech/dev-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "extract_archive(\"MaseseSpeech/train-clean.tar.xz\")\n",
    "extract_archive(\"MaseseSpeech/dev-clean.tar.xz\")\n",
    "\n",
    "!rm -r \"MaseseSpeech/train-clean.tar.xz\"\n",
    "!rm -r \"MaseseSpeech/dev-clean.tar.xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"train-clean\"\n",
    "FOLDER_IN_ARCHIVE = \"MaseseSpeech\"\n",
    "\n",
    "\n",
    "def load_masesespeech_item(fileid: str, \n",
    "                          path: str, \n",
    "                          ext_audio: str, \n",
    "                          ext_txt: str) -> Tuple[Tensor, int, str, int, int, int]:\n",
    "    \n",
    "    book_id, chapter_id, utterance_id = fileid.split(\"-\")\n",
    "    \n",
    "    file_text = book_id + \"-\" + chapter_id + ext_txt\n",
    "    file_text = os.path.join(path, book_id, chapter_id, file_text)\n",
    "    \n",
    "    fileid_audio = book_id + \"-\" + chapter_id + \"-\" + utterance_id\n",
    "    file_audio = fileid_audio + ext_audio\n",
    "    file_audio = os.path.join(path, book_id, chapter_id, file_audio)\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    try :\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(file_audio)\n",
    "\n",
    "        # Load text\n",
    "        with open(file_text) as ft:\n",
    "            for line in ft:\n",
    "                fileid_text, utterance = line.strip().split(\" \", 1) # this takes the first space split\n",
    "                if fileid_audio == fileid_text:\n",
    "                    # stop when we found the text corresponding to \n",
    "                    # the audio ID\n",
    "                    break\n",
    "            else:\n",
    "              # Translation not found\n",
    "              raise FileNotFoundError(\"Translation not found for \" + fileid_audio)\n",
    "    except:\n",
    "        print(file_audio) # this is for debugging purpose \n",
    "        print(waveform)   # to show which file may have an issue \n",
    "        pass\n",
    "#         traceback.print_exc()\n",
    "                \n",
    "    return (\n",
    "        waveform,\n",
    "        sample_rate,\n",
    "        utterance,\n",
    "        book_id,\n",
    "        chapter_id,\n",
    "        utterance_id\n",
    "        )\n",
    "\n",
    "\n",
    "class MASESESPEECH_2H_MP3(Dataset):\n",
    "    \"\"\"\n",
    "    Create a Dataset for MaseseSpeech. Each item is a tuple of the form:\n",
    "    waveform, utterance, chapter_id, verse_id, utterance_id\n",
    "    \"\"\"\n",
    "    \n",
    "    _ext_txt = \".trans.txt\"\n",
    "    _ext_audio = \".wav\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 mode: str = \"MaseseSpeech/train-clean\",\n",
    "                 folder_in_archive: str = FOLDER_IN_ARCHIVE,\n",
    "                ) -> None:\n",
    "        \n",
    "        \n",
    "        self._path = mode\n",
    "        \n",
    "        walker = walk_files(\n",
    "          self._path, suffix=self._ext_audio, prefix=False, remove_suffix=True\n",
    "        )\n",
    "        self._walker = list(walker)\n",
    "    def __getitem__(self, n: int) -> Tuple[Tensor, int, str, int, int, int]:\n",
    "        fileid = self._walker[n]\n",
    "        \n",
    "        waveform, sample_rate, utterance, book_id, chapter_id, utterance_id \\\n",
    "                    = load_masesespeech_item(fileid, self._path, self._ext_audio, self._ext_txt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        transcriptions = [list(utterance.replace(\"'\", \" \")) for b in utterance if b]\n",
    "        \n",
    "        # Use this is your acoustic model is outputting letters\n",
    "        special_caracters = \"Ɛ,;.ƆÁÓ-?:*É—“”!Í(10)\"    # Special caracters special to Lingala and this dataset\n",
    "        tokens_list = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ\"+special_caracters)\n",
    "        tokens_set = set(tokens_list)\n",
    "        \n",
    "        t = []\n",
    "        for index in transcriptions[0]:\n",
    "            t.append(str(tokens_list.index(index)))\n",
    "        \n",
    "        targets = (\" \".join(t))\n",
    "    \n",
    "        with open(\"./converted_aligned_phones.txt\", \"a+\") as text_file:              \n",
    "            # Move read cursor to the start of file.\n",
    "            text_file.seek(0)\n",
    "            # If file is not empty then append '\\n'\n",
    "            data = text_file.read(100)\n",
    "            if len(data) > 0 :\n",
    "                text_file.write(\"\\n\")\n",
    "\n",
    "            # .strip() to delect any leading and trailing whitespace\n",
    "            text_file.write(book_id+\"-\"+chapter_id+\"-\"+utterance_id+\" \"+targets)\n",
    "\n",
    "        return waveform, sample_rate, utterance,\\\n",
    "                            int(book_id), int(chapter_id), int(utterance_id)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._walker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "masese_train = MASESESPEECH_2H_MP3(\".\", mode = \"MaseseSpeech/train-clean\")\n",
    "masese_dev = MASESESPEECH_2H_MP3(\".\", mode = \"MaseseSpeech/dev-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just so you get an idea of the format \n",
    "# print(next(iter(masese_train)))\n",
    "# print(next(iter(masese_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use this is your acoustic model is outputting letters\n",
    "# special_caracters = \"Ɛ,;.ƆÁÓ-?:*É—“”!Í(10)\"    # Special caracters special to Lingala and this dataset\n",
    "# tokens_list = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ\"+special_caracters)\n",
    "# tokens_set = set(tokens_list)\n",
    "# print(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_libri(batch):\n",
    "    #print(batch)\n",
    "    tensors = [b[0].t() for b in batch if b]\n",
    "    tensors_len = [len(t) for t in tensors]\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    transcriptions = [list(b[2].replace(\"'\", \" \")) for b in batch if b]\n",
    "    targets = [torch.tensor([tokens_list.index(e) for e in t]) for t in transcriptions]\n",
    "    targets_len = [len(t) for t in targets]\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "    return tensors, targets, torch.tensor(tensors_len), torch.tensor(targets_len)\n",
    "\n",
    "# train_set = torch.utils.data.DataLoader(masese_train, batch_size=50000, shuffle=True,\n",
    "#                                         num_workers=4, collate_fn=collate_fn_libri)\n",
    "\n",
    "# train_set = torch.utils.data.DataLoader(masese_dev, batch_size=50000, shuffle=True,\n",
    "#                                         num_workers=4, collate_fn=collate_fn_libri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(train_set)))\n",
    "# print(next(iter(train_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(train_set)))\n",
    "\n",
    "\n",
    "# train_set = torch.utils.data.DataLoader(masese_train, batch_size=64, shuffle=True,\n",
    "#                                         num_workers=2, collate_fn=collate_fn_libri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CPC_audio'...\n",
      "remote: Enumerating objects: 84, done.\u001b[K\n",
      "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "remote: Total 84 (delta 13), reused 75 (delta 6), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (84/84), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/CPC_audio.git  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/AMMI-Speech-Lig-Aikuma/Dataset/Project/CPC_audio\n"
     ]
    }
   ],
   "source": [
    "%cd CPC_audio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running develop\n",
      "running egg_info\n",
      "writing CPC_audio.egg-info/PKG-INFO\n",
      "writing dependency_links to CPC_audio.egg-info/dependency_links.txt\n",
      "writing top-level names to CPC_audio.egg-info/top_level.txt\n",
      "reading manifest file 'CPC_audio.egg-info/SOURCES.txt'\n",
      "writing manifest file 'CPC_audio.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "copying build/lib.linux-x86_64-3.7/cpc/eval/ABX/dtw.cpython-37m-x86_64-linux-gnu.so -> cpc/eval/ABX\n",
      "Creating /opt/conda/lib/python3.7/site-packages/CPC-audio.egg-link (link to .)\n",
      "CPC-audio 1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /home/jupyter/AMMI-Speech-Lig-Aikuma/Dataset/Project/CPC_audio\n",
      "Processing dependencies for CPC-audio==1.0\n",
      "Finished processing dependencies for CPC-audio==1.0\n"
     ]
    }
   ],
   "source": [
    "# !%cd /CPC_audio\n",
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Building the model\n",
    "\n",
    "In this exercise, we will build and train a small CPC model using the repository CPC_audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ./CPC_audio\n",
    "from cpc.model import CPCEncoder, CPCAR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DIM_ENCODER= 256 #256\n",
    "DIM_CONTEXT= 256 #256\n",
    "KEEP_HIDDEN_VECTOR=False\n",
    "N_LEVELS_CONTEXT=1\n",
    "CONTEXT_RNN=\"LSTM\"\n",
    "N_PREDICTIONS=12\n",
    "LEARNING_RATE=2e-4\n",
    "N_NEGATIVE_SAMPLE =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CPCEncoder(DIM_ENCODER)\n",
    "context = CPCAR(DIM_ENCODER,\n",
    "                DIM_CONTEXT,\n",
    "                KEEP_HIDDEN_VECTOR,\n",
    "                N_LEVELS_CONTEXT,\n",
    "                mode=\"CONTEXT_RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get install libsndfile1-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several functions that will be necessary to load the data later\n",
    "from cpc.dataset import findAllSeqs, AudioBatchData, parseSeqLabels\n",
    "SIZE_WINDOW = 20480\n",
    "BATCH_SIZE=8\n",
    "def load_dataset(path_dataset, file_extension='.wav', phone_label_dict=None):\n",
    "    data_list, speakers = findAllSeqs(path_dataset, extension=file_extension)\n",
    "    dataset = AudioBatchData(path_dataset, SIZE_WINDOW, data_list, phone_label_dict, len(speakers))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPCModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 AR):\n",
    "\n",
    "        super(CPCModel, self).__init__()\n",
    "        self.gEncoder = encoder\n",
    "        self.gAR = AR\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        \n",
    "\n",
    "        encoder_output = self.gEncoder(batch_data)\n",
    "        #print(encoder_output.shape)\n",
    "        # The output of the encoder data does not have the good format \n",
    "        # indeed it is Batch_size x Hidden_size x temp size\n",
    "        # while the context requires Batch_size  x temp size x Hidden_size\n",
    "        # thus you need to permute\n",
    "        context_input = encoder_output.permute(0, 2, 1)\n",
    "\n",
    "        context_output = self.gAR(context_input)\n",
    "        #print(context_output.shape)\n",
    "        return context_output, encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torchaudio.load(\n",
    "#         \"../voalingala/20200611/20200611-160000-VCD361-program_16k.mp3\")[0]\n",
    "        \"../MaseseSpeech/train-clean/020/001/020-001-013.wav\")[0]\n",
    "        \n",
    "\n",
    "audio = audio.view(1, 1, -1)\n",
    "\n",
    "cpc_model = CPCModel(encoder, context).to(device)\n",
    "context_output, encoder_output = cpc_model(audio.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.1118, 0.0728, 0.0479]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cpc.dataset import parseSeqLabels\n",
    "# from cpc.feature_loader import loadModel\n",
    "\n",
    "# checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "# cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "# cpc_model = cpc_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 : CPC loss\n",
    "\n",
    "We will define a class ```CPCCriterion``` which will hold the prediction networks $\\phi_k$ defined above and perform the classification loss $\\mathcal{L}_c$.\n",
    "\n",
    "a) In this exercise, the $\\phi_k$ will be a linear transform, ie:\n",
    "\n",
    "\\\\[ \\phi_k(c_t) = \\mathbf{A}_k c_t\\\\]\n",
    "\n",
    "Using the class [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear), define the transformations $\\phi_k$ in the code below and complete the function ```get_prediction_k``` which computes $\\phi_k(c_t)$ for a given batch of vectors $c_t$.\n",
    "\n",
    "b) Using both ```get_prediction_k```  and ```sample_negatives``` defined below, write the forward function which will take as input two batches of features $c_t$ and $g_t$ and outputs the classification loss $\\mathcal{L}_c$ and the average acuracy for all predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2: write the CPC loss\n",
    "# a) Write the negative sampling (with some help)\n",
    "# ERRATUM: it's really hard, the sampling will be provided\n",
    "\n",
    "class CPCCriterion(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 K,\n",
    "                 dim_context,\n",
    "                 dim_encoder,\n",
    "                 n_negative):\n",
    "        \n",
    "        super(CPCCriterion, self).__init__()\n",
    "        \n",
    "        self.K_ = K\n",
    "        self.dim_context = dim_context\n",
    "        self.dim_encoder = dim_encoder\n",
    "        self.n_negative = n_negative\n",
    "\n",
    "        self.predictors = torch.nn.ModuleList()\n",
    "        \n",
    "        for k in range(self.K_):\n",
    "            # TO COMPLETE !\n",
    "            # A affine transformation in pytorch is equivalent to a nn.Linear layer\n",
    "            # To get a linear transformation you must set bias=False\n",
    "            # input dimension of the layer = dimension of the encoder\n",
    "            # output dimension of the layer = dimension of the context\n",
    "            self.predictors.append(torch.nn.Linear(dim_context, dim_encoder, bias=False))\n",
    "        \n",
    "    def get_prediction_k(self, context_data):\n",
    "        #TO COMPLETE !\n",
    "        output = [] \n",
    "        # For each time step k\n",
    "        for k in range(self.K_):\n",
    "            # We need to compute phi_k = A_k * c_t\n",
    "            phi_k = self.predictors[k](context_data)\n",
    "            output.append(phi_k)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def sample_negatives(self, encoded_data):\n",
    "        r\"\"\"\n",
    "        Sample some negative examples in the given encoded data.\n",
    "        Input:\n",
    "        - encoded_data size: B x T x H\n",
    "        Returns\n",
    "        - outputs of size B x (n_negative + 1) x (T - K_) x H\n",
    "          outputs[:, 0, :, :] contains the positive example\n",
    "          outputs[:, 1:, :, :] contains negative example sampled in the batch\n",
    "        - labels, long tensor of size B x (T - K_)\n",
    "          Since the positive example is always at coordinates 0 for all sequences \n",
    "          in the batch and all timestep in the sequence, labels is just a tensor\n",
    "          full of zeros !\n",
    "        \"\"\"\n",
    "        batch_size, time_size, dim_encoded = encoded_data.size()\n",
    "        window_size = time_size - self.K_\n",
    "        outputs = []\n",
    "\n",
    "        neg_ext = encoded_data.contiguous().view(-1, dim_encoded)\n",
    "        n_elem_sampled = self.n_negative * window_size * batch_size\n",
    "        # Draw nNegativeExt * batchSize negative samples anywhere in the batch\n",
    "        batch_idx = torch.randint(low=0, high=batch_size,\n",
    "                                  size=(n_elem_sampled, ),\n",
    "                                  device=encoded_data.device)\n",
    "\n",
    "        seq_idx = torch.randint(low=1, high=time_size,\n",
    "                                size=(n_elem_sampled, ),\n",
    "                                device=encoded_data.device)\n",
    "\n",
    "        base_idx = torch.arange(0, window_size, device=encoded_data.device)\n",
    "        base_idx = base_idx.view(1, 1, window_size)\n",
    "        base_idx = base_idx.expand(1, self.n_negative, window_size)\n",
    "        base_idx = base_idx.expand(batch_size, self.n_negative, window_size)\n",
    "        seq_idx += base_idx.contiguous().view(-1)\n",
    "        seq_idx = torch.remainder(seq_idx, time_size)\n",
    "\n",
    "        ext_idx = seq_idx + batch_idx * time_size\n",
    "        neg_ext = neg_ext[ext_idx].view(batch_size, self.n_negative,\n",
    "                                        window_size, dim_encoded)\n",
    "        label_loss = torch.zeros((batch_size, window_size),\n",
    "                                  dtype=torch.long,\n",
    "                                  device=encoded_data.device)\n",
    "\n",
    "        for k in range(1, self.K_ + 1):\n",
    "            # Positive samples\n",
    "            if k < self.K_:\n",
    "                pos_seq = encoded_data[:, k:-(self.K_-k)]\n",
    "            else:\n",
    "                pos_seq = encoded_data[:, k:]\n",
    "            pos_seq = pos_seq.view(batch_size, 1, pos_seq.size(1), dim_encoded)\n",
    "            full_seq = torch.cat((pos_seq, neg_ext), dim=1)\n",
    "            outputs.append(full_seq)\n",
    "\n",
    "        return outputs, label_loss\n",
    "    \n",
    "    def forward(self, encoded_data, context_data):\n",
    "\n",
    "        # TO COMPLETE:\n",
    "        # Perform the full cpc criterion\n",
    "        # Returns 2 values:\n",
    "        # - the average classification loss avg_loss\n",
    "        # - the average classification acuracy avg_acc\n",
    "\n",
    "        # Reminder : The permuation !\n",
    "        encoded_data = encoded_data.permute(0, 2, 1)\n",
    "\n",
    "        # First we need to sample the negative examples\n",
    "        negative_samples, labels = self.sample_negatives(encoded_data)\n",
    "\n",
    "        # Then we must compute phi_k\n",
    "        phi_k = self.get_prediction_k(context_data)\n",
    "\n",
    "        # Finally we must get the dot product between phi_k and negative_samples \n",
    "        # for each k\n",
    "\n",
    "        #The total loss is the average of all losses\n",
    "        avg_loss = 0\n",
    "\n",
    "        # Average acuracy\n",
    "        avg_acc = 0\n",
    "\n",
    "        for k in range(self.K_):\n",
    "            B, N_sampled, S_small, H = negative_samples[k].size() \n",
    "            B, S, H = phi_k[k].size()\n",
    "            \n",
    "            # As told before S = S_small + K. For segments too far in the sequence\n",
    "            # there are no positive exmples anyway, so we must shorten phi_k\n",
    "            phi = phi_k[k][:, :S_small]\n",
    "            \n",
    "            # Now the dot product\n",
    "            # You have several ways to do that, let's do the simple but non optimal \n",
    "            # one\n",
    "            # pytorch has a matrix product function https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "            # But it takes only 3D tensors of the same batch size !\n",
    "            # To begin negative_samples is a 4D tensor ! \n",
    "            # We want to compute the dot product for each features, of each sequence\n",
    "            # of the batch. Thus we are trying to compute a dot product for all\n",
    "            # B* N_sampled * S_small 1D vector of negative_samples[k]\n",
    "            # Or, a 1D tensor of size H is also a matrix of size 1 x H\n",
    "            # Then, we must view it as a 3D tensor of size (B* N_sampled * S_small, 1, H)\n",
    "            negative_sample_k  =  negative_samples[k].view(B* N_sampled* S_small, 1, H)\n",
    "            \n",
    "            # But now phi and negative_sample_k no longer have the same batch size !\n",
    "            # No worries, we can expand phi so that each sequence of the batch\n",
    "            # is repeated N_sampled times\n",
    "            phi = phi.view(B, 1,S_small, H).expand(B, N_sampled, S_small, H)\n",
    "            \n",
    "            # And now we can view it as a 3D tensor \n",
    "            phi  = phi.contiguous().view(B * N_sampled * S_small, H, 1)\n",
    "            \n",
    "            # We can finally get the dot product !\n",
    "            scores = torch.bmm(negative_sample_k, phi)\n",
    "            \n",
    "            # Dot_product has a size (B * N_sampled * S_small , 1, 1)\n",
    "            # Let's reorder it a bit\n",
    "            scores = scores.reshape(B, N_sampled, S_small)\n",
    "            \n",
    "            # For each elements of the sequence, and each elements sampled, it gives \n",
    "            # a floating score stating the likelihood of this element being the \n",
    "            # true one.\n",
    "            # Now the classification loss, we need to use the Cross Entropy loss\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html\n",
    "            \n",
    "            # For each time-step of each sequence of the batch \n",
    "            # we have N_sampled possible predictions. \n",
    "            # Looking at the documentation of torch.nn.CrossEntropyLoss\n",
    "            # we can see that this loss expect a tensor of size M x C where \n",
    "            # - M is the number of elements with a classification score\n",
    "            # - C is the number of possible classes\n",
    "            # There are N_sampled candidates for each predictions so\n",
    "            # C = N_sampled \n",
    "            # Each timestep of each sequence of the batch has a prediction so\n",
    "            # M = B * S_small\n",
    "            # Thus we need an input vector of size B * S_small, N_sampled\n",
    "            # To begin, we need to permute the axis\n",
    "            scores = scores.permute(0, 2, 1) # Now it has size B , S_small, N_sampled\n",
    "            \n",
    "            # Then we can cast it into a 2D tensor\n",
    "            scores = scores.reshape(B * S_small, N_sampled)\n",
    "            \n",
    "            # Same thing for the labels \n",
    "            labels = labels.reshape(B * S_small)\n",
    "            \n",
    "            # Finally we can get the classification loss\n",
    "            loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "            loss_k = loss_criterion(scores, labels)\n",
    "            avg_loss+= loss_k\n",
    "            \n",
    "            # And for the acuracy\n",
    "            # The prediction for each elements is the sample with the highest score\n",
    "            # Thus the tensors of all predictions is the tensors of the index of the \n",
    "            # maximal score for each time-step of each sequence of the batch\n",
    "            predictions = torch.argmax(scores, 1)\n",
    "            acc_k  = (labels == predictions).sum() / (B * S_small)\n",
    "            avg_acc += acc_k\n",
    "\n",
    "        # Normalization\n",
    "        avg_loss = avg_loss / self.K_\n",
    "        avg_acc = avg_acc / self.K_\n",
    "        \n",
    "        return avg_loss , avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "audio = torchaudio.load(\n",
    "    \"../MaseseSpeech/train-clean/020/001/020-001-013.wav\")[0]\n",
    "#     \"../voalingala/20200611/20200611-160000-VCD361-program_16k.mp3\")[0]\n",
    "\n",
    "audio = audio.view(1, 1, -1)\n",
    "cpc_criterion = CPCCriterion(N_PREDICTIONS, DIM_CONTEXT, \n",
    "                             DIM_ENCODER, N_NEGATIVE_SAMPLE).to(device)\n",
    "context_output, encoder_output = cpc_model(audio.to(device))\n",
    "loss, avg = cpc_criterion(encoder_output,context_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3173, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Full training loop !\n",
    "\n",
    "You have the model, you have the criterion. All you need now are a data loader and an optimizer to run your training loop.\n",
    "\n",
    "We will use an Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(cpc_criterion.parameters()) + list(cpc_model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 4370.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache file at ../MaseseSpeech/train-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "363it [00:00, 890890.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking length...\n",
      "Done, elapsed: 0.109 seconds\n",
      "Scanned 363 sequences in 0.11 seconds\n",
      "1 chunks computed\n",
      "Joining pool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a89c58ee2912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                              file_extension='.mp3')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../MaseseSpeech/train-clean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdataset_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../MaseseSpeech/dev-clean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-83ac37e99d77>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path_dataset, file_extension, phone_label_dict)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_label_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeakers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindAllSeqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioBatchData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIZE_WINDOW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_label_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeakers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/AMMI-Speech-Lig-Aikuma/Dataset/Project/CPC_audio/cpc/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, sizeWindow, seqNames, phoneLabelsDict, nSpeakers, nProcessLoader, MAX_SIZE_LOADED)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneLabelsDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoneLabelsDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadNextPack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadNextPack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoubleLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/AMMI-Speech-Lig-Aikuma/Dataset/Project/CPC_audio/cpc/dataset.py\u001b[0m in \u001b[0;36mloadNextPack\u001b[0;34m(self, first)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Joining pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Joined process, elapsed={time.time()-start_time:.3f} secs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dataset_train = load_dataset('../voalingala/train',\n",
    "#                              file_extension='.mp3')\n",
    "# dataset_val = load_dataset('../voalingala/val',\n",
    "#                              file_extension='.mp3')\n",
    "\n",
    "dataset_train = load_dataset('../MaseseSpeech/train-clean')\n",
    "dataset_val = load_dataset('../MaseseSpeech/dev-clean')\n",
    "\n",
    "# data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n",
    "data_loader_val = dataset_train.getDataLoader(BATCH_SIZE, \"sequence\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(data_loader,\n",
    "               cpc_model,\n",
    "               cpc_criterion,\n",
    "               optimizer):\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    n_items = 0\n",
    "    \n",
    "    for step, data in enumerate(data_loader):\n",
    "        x,y = data\n",
    "        bs = len(x)\n",
    "        optimizer.zero_grad()\n",
    "        context_output, encoder_output = cpc_model(x.to(device))\n",
    "        loss , acc = cpc_criterion(encoder_output, context_output)\n",
    "        loss.backward()\n",
    "        n_items+=bs\n",
    "        avg_loss+=loss.item()*bs\n",
    "        avg_acc +=acc.item()*bs\n",
    "    avg_loss/=n_items\n",
    "    avg_acc/=n_items\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 : Validation loop\n",
    "\n",
    "Now complete the validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(data_loader,\n",
    "                    cpc_model,\n",
    "                    cpc_criterion):\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    n_items = 0\n",
    "    \n",
    "    for step, data in enumerate(data_loader):\n",
    "        x,y = data\n",
    "        bs = len(x)\n",
    "        context_output, encoder_output = cpc_model(x.to(device))\n",
    "        loss , acc = cpc_criterion(encoder_output, context_output)\n",
    "        n_items+=bs\n",
    "        avg_loss+=loss.item()*bs\n",
    "        avg_acc+=acc.item()*bs\n",
    "        \n",
    "    avg_loss/=n_items\n",
    "    avg_acc/=n_items\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBVUPKKs2_0U"
   },
   "source": [
    "## Exercise 5: Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_loader,\n",
    "        val_loader,\n",
    "        cpc_model,\n",
    "        cpc_criterion,\n",
    "        optimizer,\n",
    "        n_epochs):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Running epoch {epoch+1} / {n_epochs}\")\n",
    "        avg_loss_train, avg_acc_train = train_step(train_loader, cpc_model, cpc_criterion, optimizer)\n",
    "        print(\"----------------------\")\n",
    "        print(f\"Training dataset\")\n",
    "        print(f\"- average loss : {avg_loss_train}\")\n",
    "        print(f\"- average acuracy : {avg_acc_train}\")\n",
    "        print(\"----------------------\")\n",
    "        with torch.no_grad():\n",
    "            cpc_model.eval()\n",
    "            cpc_criterion.eval()\n",
    "            avg_loss_val, avg_acc_val = validation_step(val_loader, cpc_model, cpc_criterion)\n",
    "            print(f\"Validation dataset\")\n",
    "            print(f\"- average loss : {avg_loss_val}\")\n",
    "            print(f\"- average acuracy : {avg_acc_val}\")\n",
    "            print(\"----------------------\")\n",
    "            print()\n",
    "            cpc_model.train()\n",
    "            cpc_criterion.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(data_loader_train, data_loader_val, cpc_model,cpc_criterion,optimizer,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is donw, clear the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dataset_train\n",
    "# del dataset_val\n",
    "# del cpc_model\n",
    "# del context\n",
    "# del encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srPM5r_LB9v-"
   },
   "source": [
    "# Part 2 : Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Nb_-0IQiJk9"
   },
   "source": [
    "## Exercice 1 : Phone separability with aligned phonemes.\n",
    "\n",
    "One option to **evaluate the quality of the features trained with CPC can be to check if they can be used to recognize phonemes**. \n",
    "To do so, we can fine-tune a **pre-trained model using a limited amount of labelled speech data**.\n",
    "We are going to start with a simple evaluation setting where we have the phone labels for each timestep corresponding to a CPC feature.\n",
    "\n",
    "We will work with a model already pre-trained on English data. As far as the fine-tuning dataset is concerned, we will use a 1h subset of [librispeech-100](http://www.openslr.org/12/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "N-scDMAasXxc",
    "outputId": "3ae4d269-ccba-405e-ad08-da80e85a7e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoint_data’: File exists\n",
      "--2020-07-05 18:42:36--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_30.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 113599715 (108M) [application/octet-stream]\n",
      "Saving to: ‘checkpoint_data/checkpoint_30.pt.1’\n",
      "\n",
      "checkpoint_30.pt.1  100%[===================>] 108.34M  84.1MB/s    in 1.3s    \n",
      "\n",
      "2020-07-05 18:42:37 (84.1 MB/s) - ‘checkpoint_data/checkpoint_30.pt.1’ saved [113599715/113599715]\n",
      "\n",
      "--2020-07-05 18:42:37--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_logs.json\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20786 (20K) [text/plain]\n",
      "Saving to: ‘checkpoint_data/checkpoint_logs.json.1’\n",
      "\n",
      "checkpoint_logs.jso 100%[===================>]  20.30K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2020-07-05 18:42:38 (2.31 MB/s) - ‘checkpoint_data/checkpoint_logs.json.1’ saved [20786/20786]\n",
      "\n",
      "--2020-07-05 18:42:38--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_args.json\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2063 (2.0K) [text/plain]\n",
      "Saving to: ‘checkpoint_data/checkpoint_args.json.1’\n",
      "\n",
      "checkpoint_args.jso 100%[===================>]   2.01K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-07-05 18:42:38 (29.6 MB/s) - ‘checkpoint_data/checkpoint_args.json.1’ saved [2063/2063]\n",
      "\n",
      "checkpoint_30.pt    checkpoint_args.json    checkpoint_logs.json\n",
      "checkpoint_30.pt.1  checkpoint_args.json.1  checkpoint_logs.json.1\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoint_data\n",
    "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_30.pt -P checkpoint_data\n",
    "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_logs.json -P checkpoint_data\n",
    "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_args.json -P checkpoint_data\n",
    "!ls checkpoint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m020\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../MaseseSpeech/dev-clean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "SSSaiYo82_oY",
    "outputId": "cbd1917b-6d69-4eaf-a564-62f52aa9ba6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
      "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 10010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2f9f6785af0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoint_data/checkpoint_30.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcpc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_CONTEXT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_ENCODER_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcpc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_PHONES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseSeqLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../MaseseSpeech/converted_aligned_phones.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m             raise RuntimeError(\n\u001b[1;32m    148\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise AssertionError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mAlternatively\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m of the CUDA driver.\"\"\".format(str(torch._C._cuda_getDriverVersion())))\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 10010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "# %cd /content/CPC_audio\n",
    "from cpc.dataset import parseSeqLabels\n",
    "from cpc.feature_loader import loadModel\n",
    "\n",
    "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "cpc_model = cpc_model.cuda()\n",
    "\n",
    "label_dict, N_PHONES = parseSeqLabels('../MaseseSpeech/converted_aligned_phones.txt')\n",
    "dataset_train = load_dataset('../MaseseSpeech/train-clean/', file_extension='.wav', phone_label_dict=label_dict)\n",
    "dataset_val = load_dataset('../MaseseSpeech/dev-clean/', file_extension='.wav', phone_label_dict=label_dict)\n",
    "\n",
    "data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n",
    "data_loader_val = dataset_val.getDataLoader(BATCH_SIZE, \"sequence\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ulgYV3nHcoa"
   },
   "outputs": [],
   "source": [
    "# ??cpc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkKi-qfosng2"
   },
   "source": [
    "Then we will use a simple linear classifier to recognize the phonemes from the features produced by ```cpc_model```. \n",
    "\n",
    "### a) Build the phone classifier \n",
    "\n",
    "Design a class of linear classifiers, ```PhoneClassifier``` that will take as input a batch of sequences of CPC features and output a score vector for each phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RpAbz-0CXJJ"
   },
   "outputs": [],
   "source": [
    "class PhoneClassifier(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim : int,\n",
    "                 n_phones : int):\n",
    "        super(PhoneClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, n_phones)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zt5oa_nqtH-d"
   },
   "source": [
    "Our phone classifier will then be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRBf_83IuLv5"
   },
   "outputs": [],
   "source": [
    "phone_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_PHONES).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_Vf5AbUhqm4"
   },
   "source": [
    "### b - What would be the correct loss criterion for this task ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhyPM-cgjrtw"
   },
   "outputs": [],
   "source": [
    "loss_criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nv4cSxbaplrz"
   },
   "source": [
    "To perform the fine-tuning, we will also need an optimization function.\n",
    "\n",
    "We will use an [Adam optimizer ](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5CgYyAlqKxu"
   },
   "outputs": [],
   "source": [
    "parameters = list(phone_classifier.parameters()) + list(cpc_model.parameters())\n",
    "LEARNING_RATE = 2e-4\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQB9HS9PvAXc"
   },
   "source": [
    "You might also want to perform this training while freezing the weights of the ```cpc_model```. Indeed, if the pre-training was good enough, then ```cpc_model``` phonemes representation should be linearly separable. In this case the optimizer should be defined like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRy0gn6awGUQ"
   },
   "outputs": [],
   "source": [
    "optimizer_frozen = torch.optim.Adam(list(phone_classifier.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cO93ngIfj4JW"
   },
   "source": [
    "### c- Now let's build a training loop. \n",
    "Complete the function ```train_one_epoch``` below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fabqj3wvLwgU"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(cpc_model, \n",
    "                    phone_classifier, \n",
    "                    loss_criterion, \n",
    "                    data_loader, \n",
    "                    optimizer):\n",
    "    \n",
    "    cpc_model.train()\n",
    "    loss_criterion.train()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    n_items = 0\n",
    "    for step, full_data in enumerate(data_loader):\n",
    "        # Each batch is represented by a Tuple of vectors:\n",
    "        # sequence of size : N x 1 x T\n",
    "        # label of size : N x T\n",
    "        # \n",
    "        # With :\n",
    "        # - N number of sequence in the batch\n",
    "        # - T size of each sequence\n",
    "        sequence, label = full_data\n",
    "\n",
    "\n",
    "\n",
    "        bs = len(sequence)\n",
    "        seq_len = label.size(1)\n",
    "        optimizer.zero_grad()\n",
    "        context_out, enc_out, _ = cpc_model(sequence.to(device),label.to(device))\n",
    "\n",
    "        scores = phone_classifier(context_out)\n",
    "\n",
    "        scores = scores.permute(0,2,1)\n",
    "        loss = loss_criterion(scores,label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss+=loss.item()*bs\n",
    "        n_items+=bs\n",
    "        correct_labels = scores.argmax(1)\n",
    "        avg_accuracy += ((label==correct_labels.cpu()).float()).mean(1).sum().item()\n",
    "        \n",
    "    avg_loss/=n_items\n",
    "    avg_accuracy/=n_items\n",
    "    \n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quYtjx_TxIPK"
   },
   "source": [
    "Don't forget to test it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50MwxbKhxMKp"
   },
   "outputs": [],
   "source": [
    "avg_loss, avg_accuracy = train_one_epoch(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_o6yk8XKWnYe",
    "outputId": "19b006e6-03ac-4c18-c415-cdeb293b54d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3621995804512956, 0.6497440501715266)"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmUkuJ2bwu4Z"
   },
   "source": [
    "### d- Build the validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZJMxj6cwzd3"
   },
   "outputs": [],
   "source": [
    "def validation_step(cpc_model, \n",
    "                    phone_classifier, \n",
    "                    loss_criterion, \n",
    "                    data_loader):\n",
    "    \n",
    "    cpc_model.eval()\n",
    "    phone_classifier.eval()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    n_items = 0\n",
    "    with torch.no_grad():\n",
    "        for step, full_data in enumerate(data_loader):\n",
    "            # Each batch is represented by a Tuple of vectors:\n",
    "            # sequence of size : N x 1 x T\n",
    "            # label of size : N x T\n",
    "            # \n",
    "            # With :\n",
    "            # - N number of sequence in the batch\n",
    "            # - T size of each sequence\n",
    "            sequence, label = full_data\n",
    "            bs = len(sequence)\n",
    "            seq_len = label.size(1)\n",
    "            context_out, enc_out, _ = cpc_model(sequence.to(device),label.to(device))\n",
    "            scores = phone_classifier(context_out)\n",
    "            scores = scores.permute(0,2,1)\n",
    "            loss = loss_criterion(scores,label.to(device))\n",
    "            avg_loss+=loss.item()*bs\n",
    "            n_items+=bs\n",
    "            correct_labels = scores.argmax(1)\n",
    "            avg_accuracy += ((label==correct_labels.cpu()).float()).mean(1).sum().item()\n",
    "            \n",
    "    avg_loss/=n_items\n",
    "    avg_accuracy/=n_items\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vownVCt7xbVh"
   },
   "source": [
    "### e- Run everything\n",
    "\n",
    "Test this functiion with both ```optimizer``` and ```optimizer_frozen```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvO_4nKUxfQx"
   },
   "outputs": [],
   "source": [
    "def run(cpc_model, \n",
    "        phone_classifier, \n",
    "        loss_criterion, \n",
    "        data_loader_train, \n",
    "        data_loader_val, \n",
    "        optimizer,\n",
    "        n_epoch):\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        print(f\"Running epoch {epoch + 1} / {n_epoch}\")\n",
    "        loss_train, acc_train = train_one_epoch(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer)\n",
    "        print(\"-------------------\")\n",
    "        print(f\"Training dataset :\")\n",
    "        print(f\"Average loss : {loss_train}. Average accuracy {acc_train}\")\n",
    "\n",
    "        print(\"-------------------\")\n",
    "        print(\"Validation dataset\")\n",
    "        loss_val, acc_val = validation_step(cpc_model, phone_classifier, loss_criterion, data_loader_val)\n",
    "        print(f\"Average loss : {loss_val}. Average accuracy {acc_val}\")\n",
    "        print(\"-------------------\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ceCEO2h2bxAn",
    "outputId": "2842125e-7f20-434d-9189-33699670f620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 1.0219527069604895. Average accuracy 0.7057498257933105\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 1.0688188383544701. Average accuracy 0.6966259057971015\n",
      "-------------------\n",
      "\n",
      "Running epoch 2 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 0.9813667901793229. Average accuracy 0.7150101397226987\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 1.0425163616304811. Average accuracy 0.7018257472826087\n",
      "-------------------\n",
      "\n",
      "Running epoch 3 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 0.9586697046657233. Average accuracy 0.7197957984919954\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 1.029444672577623. Average accuracy 0.7042572463768116\n",
      "-------------------\n",
      "\n",
      "Running epoch 4 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 0.9506696053679975. Average accuracy 0.7210777810534591\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 0.9972989958265553. Average accuracy 0.7106855751811594\n",
      "-------------------\n",
      "\n",
      "Running epoch 5 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 0.9448814161032387. Average accuracy 0.7218349101272156\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 0.9780038367147031. Average accuracy 0.7143144248188406\n",
      "-------------------\n",
      "\n",
      "Running epoch 6 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 0.929420669805397. Average accuracy 0.7249711888936535\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 0.9746506056923797. Average accuracy 0.715234375\n",
      "-------------------\n",
      "\n",
      "Running epoch 7 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 0.9157625487901743. Average accuracy 0.7281409689465409\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 0.9955525258313055. Average accuracy 0.710739356884058\n",
      "-------------------\n",
      "\n",
      "Running epoch 8 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 0.9135043791459587. Average accuracy 0.7293961504788451\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 0.9721649721048881. Average accuracy 0.7155910326086956\n",
      "-------------------\n",
      "\n",
      "Running epoch 9 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 0.9004762198940013. Average accuracy 0.7319076302530018\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 0.980453244154004. Average accuracy 0.713963428442029\n",
      "-------------------\n",
      "\n",
      "Running epoch 10 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 0.9172716077018834. Average accuracy 0.7282263972269869\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 0.9502466599146525. Average accuracy 0.7199048913043479\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(cpc_model,phone_classifier,loss_criterion,data_loader_train,data_loader_val,optimizer_frozen,n_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TdfWDiFnylMT"
   },
   "source": [
    "## Exercise 2 : Phone separability without alignment (PER)\n",
    "\n",
    "**Aligned data are very practical, but un real life they are rarely available.** That's why in this excercise we will consider a **fine-tuning with non-aligned phonemes.**\n",
    "\n",
    "The model, the optimizer and the phone classifier will stay the same. However, we will replace our phone criterion with a [CTC loss](https://pytorch.org/docs/master/generated/torch.nn.CTCLoss.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9BpM_Lpzgx8"
   },
   "outputs": [],
   "source": [
    "loss_ctc = torch.nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQpYgTyfzsrq"
   },
   "source": [
    "Besides, we will use a siglthy different dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "9HRxoatlz3ZZ",
    "outputId": "210b7f5f-b722-42dd-97df-56cb2e614a1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [00:00, 16115.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/CPC_audio\n",
      "Saved cache file at /content/per_data/pack_master/1h/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [00:00, 1867.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 287 sequences in 4.05 seconds\n",
      "maxSizeSeq : 273359\n",
      "maxSizePhone : 207\n",
      "minSizePhone : 17\n",
      "Total size dataset 1.0406152430555555 hours\n",
      "Saved cache file at /content/per_data/pack_master/10min/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 212 sequences in 2.49 seconds\n",
      "maxSizeSeq : 273760\n",
      "maxSizePhone : 188\n",
      "minSizePhone : 17\n",
      "Total size dataset 0.73 hours\n"
     ]
    }
   ],
   "source": [
    "%cd /content/CPC_audio\n",
    "from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n",
    "path_train_data_per = '/content/per_data/pack_master/1h'\n",
    "path_val_data_per = '/content/per_data/pack_master/10min'\n",
    "path_phone_data_per = '/content/per_data/pack_master/10h_phones.txt'\n",
    "BATCH_SIZE=8\n",
    "\n",
    "phone_labels, N_PHONES = parseSeqLabels(path_phone_data_per)\n",
    "data_train_per, _ = findAllSeqs(path_train_data_per, extension='.flac')\n",
    "dataset_train_non_aligned = SingleSequenceDataset(path_train_data_per, data_train_per, phone_labels)\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True)\n",
    "\n",
    "data_val_per, _ = findAllSeqs(path_val_data_per, extension='.flac')\n",
    "dataset_val_non_aligned = SingleSequenceDataset(path_val_data_per, data_val_per, phone_labels)\n",
    "data_loader_val = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwAckY62z7s9"
   },
   "source": [
    "### a- Training\n",
    "\n",
    "Since the phonemes are not aligned, there is no simple direct way to get the classification acuracy of a model. Write and test the three functions ```train_one_epoch_ctc```, ```validation_step_ctc``` and ```run_ctc``` as before but without considering the average acuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oYg5YzW8EHl4",
    "outputId": "be8aec47-f0a8-4cb6-b4ff-df232118a6d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
      "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
     ]
    }
   ],
   "source": [
    "from cpc.feature_loader import loadModel\n",
    "\n",
    "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "cpc_model = cpc_model.cuda()\n",
    "phone_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_PHONES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFQ2g3PjErdZ"
   },
   "outputs": [],
   "source": [
    "parameters = list(phone_classifier.parameters()) + list(cpc_model.parameters())\n",
    "LEARNING_RATE = 2e-4\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "\n",
    "optimizer_frozen = torch.optim.Adam(list(phone_classifier.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zsgjv3cD0oqD"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_one_epoch_ctc(cpc_model, \n",
    "                        phone_classifier, \n",
    "                        loss_criterion, \n",
    "                        data_loader, \n",
    "                        optimizer):\n",
    "    \n",
    "    cpc_model.train()\n",
    "    loss_criterion.train()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    n_items = 0\n",
    "    for step, full_data in enumerate(data_loader):\n",
    "\n",
    "        x, x_len, y, y_len = full_data\n",
    "\n",
    "        x_batch_len = x.shape[-1]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        bs=x.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        context_out, enc_out, _ = cpc_model(x.to(device),y.to(device))\n",
    "\n",
    "        scores = phone_classifier(context_out)\n",
    "        scores = scores.permute(1,0,2)\n",
    "        scores = F.log_softmax(scores,2)\n",
    "        yhat_len = torch.tensor([int(scores.shape[0]*x_len[i]/x_batch_len) for i in range(scores.shape[1])]) # this is an approximation, should be good enough\n",
    "\n",
    "        loss = loss_criterion(scores,y.to(device),yhat_len,y_len)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss+=loss.item()*bs\n",
    "        n_items+=bs\n",
    "        \n",
    "    avg_loss/=n_items\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def validation_step(cpc_model, \n",
    "                    phone_classifier, \n",
    "                    loss_criterion, \n",
    "                    data_loader):\n",
    "    \n",
    "    cpc_model.eval()\n",
    "    phone_classifier.eval()\n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    n_items = 0\n",
    "    with torch.no_grad():\n",
    "        for step, full_data in enumerate(data_loader):\n",
    "            \n",
    "            x, x_len, y, y_len = full_data\n",
    "            x_batch_len = x.shape[-1]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            bs=x.size(0)\n",
    "            context_out, enc_out, _ = cpc_model(x.to(device),y.to(device))\n",
    "            \n",
    "            scores = phone_classifier(context_out)\n",
    "            scores = scores.permute(1,0,2)\n",
    "            scores = F.log_softmax(scores,2)\n",
    "            yhat_len = torch.tensor([int(scores.shape[0]*x_len[i]/x_batch_len) for i in range(scores.shape[1])]) # this is an approximation, should be good enough\n",
    "            \n",
    "            loss = loss_criterion(scores,y.to(device),yhat_len,y_len)\n",
    "            avg_loss+=loss.item()*bs\n",
    "            n_items+=bs\n",
    "            \n",
    "    avg_loss/=n_items\n",
    "    return avg_loss\n",
    "\n",
    "def run_ctc(cpc_model, \n",
    "            phone_classifier, \n",
    "            loss_criterion, \n",
    "            data_loader_train, \n",
    "            data_loader_val, \n",
    "            optimizer,\n",
    "            n_epoch):\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        print(f\"Running epoch {epoch + 1} / {n_epoch}\")\n",
    "        loss_train = train_one_epoch_ctc(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer)\n",
    "        print(\"-------------------\")\n",
    "        print(f\"Training dataset :\")\n",
    "        print(f\"Average loss : {loss_train}.\")\n",
    "\n",
    "        print(\"-------------------\")\n",
    "        print(\"Validation dataset\")\n",
    "        loss_val = validation_step(cpc_model, phone_classifier, loss_criterion, data_loader_val)\n",
    "        print(f\"Average loss : {loss_val}\")\n",
    "        print(\"-------------------\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GSr7tcUdD72c",
    "outputId": "ea9f1f53-6b42-436d-e760-68bfdae72bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 32.44543953208657.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 32.01081585093132\n",
      "-------------------\n",
      "\n",
      "Running epoch 2 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 30.99022026328774.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 30.300324444522225\n",
      "-------------------\n",
      "\n",
      "Running epoch 3 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 29.319565432888645.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 28.464903420181635\n",
      "-------------------\n",
      "\n",
      "Running epoch 4 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 27.567655403297262.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 26.642856191119876\n",
      "-------------------\n",
      "\n",
      "Running epoch 5 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 25.832834390493538.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 24.82515669546986\n",
      "-------------------\n",
      "\n",
      "Running epoch 6 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 24.127855487636754.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 23.089440377402646\n",
      "-------------------\n",
      "\n",
      "Running epoch 7 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 22.487650171026484.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 21.420966125777547\n",
      "-------------------\n",
      "\n",
      "Running epoch 8 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 20.911723703771205.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 19.82448653813222\n",
      "-------------------\n",
      "\n",
      "Running epoch 9 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 19.407203647640202.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 18.315616182806366\n",
      "-------------------\n",
      "\n",
      "Running epoch 10 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 17.984150039566146.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 16.896408695745244\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_ctc(cpc_model,phone_classifier,loss_ctc,data_loader_train,data_loader_val,optimizer_frozen,n_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TKrYW4gK1BBF"
   },
   "source": [
    "### b- Evaluation: the Phone Error Rate (PER)\n",
    "\n",
    "In order to compute the similarity between two sequences, we can use the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). This distance estimates the minimum number of insertion, deletion and addition to move from one sequence to another. If we normalize this distance by the number of characters in the reference sequence we get the Phone Error Rate (PER).\n",
    "\n",
    "This value can be interpreted as :\n",
    "\\\\[  PER = \\frac{S + D + I}{N} \\\\]\n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "*   N is the number of characters in the reference\n",
    "*   S is the number of substitutiion\n",
    "*   I in the number of insertion\n",
    "*   D in the number of deletion\n",
    "\n",
    "For the best possible alignment of the two sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RoBhsx7GNqI_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_PER_sequence(ref_seq, target_seq):\n",
    "    \n",
    "    # re = g.split()\n",
    "    # h = h.split()\n",
    "    n = len(ref_seq)\n",
    "    m = len(target_seq)\n",
    "    \n",
    "    D = np.zeros((n+1,m+1))\n",
    "    for i in range(1,n+1):\n",
    "        D[i,0] = D[i-1,0]+1\n",
    "    for j in range(1,m+1):\n",
    "        D[0,j] = D[0,j-1]+1\n",
    "        \n",
    "    ### TODO compute the alignment\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            D[i,j] = min(\n",
    "                D[i-1,j]+1,\n",
    "                D[i-1,j-1]+1,\n",
    "                D[i,j-1]+1,\n",
    "                D[i-1,j-1]+ 0 if ref_seq[i-1]==target_seq[j-1] else float(\"inf\")\n",
    "            )\n",
    "            \n",
    "    return D[n,m]/len(ref_seq)\n",
    "\n",
    "    #return PER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-hr0KK0mgcR"
   },
   "source": [
    "You can test your function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AfTb3yOQmvey",
    "outputId": "5db807ca-4c58-4413-ab19-6581d7d07263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ref_seq = [0, 1, 1, 2, 0, 2, 2]\n",
    "pred_seq = [1, 1, 2, 2, 0, 0]\n",
    "\n",
    "expected_PER = 4. / 7.\n",
    "print(get_PER_sequence(ref_seq, pred_seq) == expected_PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHiyChl-m_k7"
   },
   "source": [
    "## c- Evaluating the PER of your model on the test dataset\n",
    "\n",
    "Evaluate the PER on the validation dataset. Please notice that you should usually use a separate dataset, called the dev dataset, to perform this operation. However for the sake of simplicity we will work with validation data in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DMkX0PoFnclg"
   },
   "outputs": [],
   "source": [
    "import progressbar\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def cut_data(seq, sizeSeq):\n",
    "    maxSeq = sizeSeq.max()\n",
    "    return seq[:, :maxSeq]\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    seq, sizeSeq, phone, sizePhone = data\n",
    "    seq = seq.cuda()\n",
    "    phone = phone.cuda()\n",
    "    sizeSeq = sizeSeq.cuda().view(-1)\n",
    "    sizePhone = sizePhone.cuda().view(-1)\n",
    "\n",
    "    seq = cut_data(seq.permute(0, 2, 1), sizeSeq).permute(0, 2, 1)\n",
    "    return seq, sizeSeq, phone, sizePhone\n",
    "\n",
    "\n",
    "def get_per(test_dataloader,\n",
    "            cpc_model,\n",
    "            phone_classifier):\n",
    "    \n",
    "    downsampling_factor = 160\n",
    "    cpc_model.eval()\n",
    "    phone_classifier.eval()\n",
    "    \n",
    "    avgPER = 0\n",
    "    nItems = 0 \n",
    "    \n",
    "    print(\"Starting the PER computation through beam search\")\n",
    "    bar = progressbar.ProgressBar(maxval=len(test_dataloader))\n",
    "    bar.start()\n",
    "    \n",
    "    for index, data in enumerate(test_dataloader):\n",
    "\n",
    "        bar.update(index)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            seq, sizeSeq, phone, sizePhone = prepare_data(data)\n",
    "            c_feature, _, _ = cpc_model(seq.to(device),phone.to(device))\n",
    "            sizeSeq = sizeSeq / downsampling_factor\n",
    "            predictions = torch.nn.functional.softmax(\n",
    "            phone_classifier(c_feature), dim=2).cpu()\n",
    "            phone = phone.cpu()\n",
    "            sizeSeq = sizeSeq.cpu()\n",
    "            sizePhone = sizePhone.cpu()\n",
    "\n",
    "            bs = c_feature.size(0)\n",
    "            data_per = [(predictions[b].argmax(1),  phone[b]) for b in range(bs)]\n",
    "            # data_per = [(predictions[b], sizeSeq[b], phone[b], sizePhone[b],\n",
    "            #               \"criterion.module.BLANK_LABEL\") for b in range(bs)]\n",
    "\n",
    "            with Pool(bs) as p:\n",
    "                poolData = p.starmap(get_PER_sequence, data_per)\n",
    "            avgPER += sum([x for x in poolData])\n",
    "            nItems += len(poolData)\n",
    "            \n",
    "    bar.finish()\n",
    "    \n",
    "    avgPER /= nItems\n",
    "\n",
    "    print(f\"Average PER {avgPER}\")\n",
    "    \n",
    "    return avgPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "2hvnudh4Osb4",
    "outputId": "acdd8ab7-c766-4a2b-9b6c-b744266d6240"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 27) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the PER computation through beam search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (27 of 27) |########################| Elapsed Time: 0:10:44 Time:  0:10:44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PER 0.9509821691500522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9509821691500522"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_per(data_loader_val,cpc_model,phone_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8e9D7g8159k"
   },
   "source": [
    "## Exercice 3 : Character error rate (CER) \n",
    "\n",
    "**The Character Error Rate (CER) is an evaluation metric similar to the PER but with characters insterad of phonemes.** Using the following data, run the functions you defined previously to estimate the CER of your model after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "cXONmKQOuFSn",
    "outputId": "5a2791ec-ff0e-4749-ce93-92922ffbee01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [00:00, 10784.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/CPC_audio\n",
      "Saved cache file at /content/per_data/pack_master/1h/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [00:00, 1741.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 287 sequences in 2.68 seconds\n",
      "maxSizeSeq : 273359\n",
      "maxSizePhone : 300\n",
      "minSizePhone : 18\n",
      "Total size dataset 1.0406152430555555 hours\n",
      "Saved cache file at /content/per_data/pack_master/10min/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 212 sequences in 2.29 seconds\n",
      "maxSizeSeq : 273760\n",
      "maxSizePhone : 273\n",
      "minSizePhone : 29\n",
      "Total size dataset 0.73 hours\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset labelled with the letters of each sequence.\n",
    "# %cd /content/CPC_audio\n",
    "from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n",
    "# path_train_data_cer = '/content/per_data/pack_master/1h'\n",
    "# path_val_data_cer = '/content/per_data/pack_master/10min'\n",
    "# path_letter_data_cer = '/content/per_data/pack_master/chars.txt'\n",
    "\n",
    "BATCH_SIZE=8\n",
    "\n",
    "letters_labels, N_LETTERS = parseSeqLabels(path_letter_data_cer)\n",
    "data_train_cer, _ = findAllSeqs(path_train_data_cer, extension='.flac')\n",
    "dataset_train_non_aligned = SingleSequenceDataset(path_train_data_cer, data_train_cer, letters_labels)\n",
    "\n",
    "\n",
    "data_val_cer, _ = findAllSeqs(path_val_data_cer, extension='.flac')\n",
    "dataset_val_non_aligned = SingleSequenceDataset(path_val_data_cer, data_val_cer, letters_labels)\n",
    "\n",
    "\n",
    "# The data loader will generate a tuple of tensors data, labels for each batch\n",
    "# data : size N x T1 x 1 : the audio sequence\n",
    "# label : size N x T2 the sequence of letters corresponding to the audio data\n",
    "# IMPORTANT NOTE: just like the PER the CER is computed with non-aligned phone data.\n",
    "data_loader_train_letters = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True)\n",
    "data_loader_val_letters = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9h07zI2LjzAU",
    "outputId": "70542ed5-a32e-448f-83ea-6355e601511e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
      "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
     ]
    }
   ],
   "source": [
    "from cpc.feature_loader import loadModel\n",
    "\n",
    "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "cpc_model = cpc_model.cuda()\n",
    "character_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_LETTERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHCNg1E7lW1L"
   },
   "outputs": [],
   "source": [
    "parameters = list(character_classifier.parameters()) + list(cpc_model.parameters())\n",
    "LEARNING_RATE = 2e-4\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "\n",
    "optimizer_frozen = torch.optim.Adam(list(character_classifier.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "engpkljbk9hj"
   },
   "outputs": [],
   "source": [
    "loss_ctc = torch.nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9NBHd2s2kxld",
    "outputId": "24067818-b4b2-4f90-a433-a63f851b7587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 17.15224294729166.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 16.621312869103598\n",
      "-------------------\n",
      "\n",
      "Running epoch 2 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 15.531890602378578.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 14.840831449246519\n",
      "-------------------\n",
      "\n",
      "Running epoch 3 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 13.80899079863008.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 12.998893340052021\n",
      "-------------------\n",
      "\n",
      "Running epoch 4 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 12.093861906678526.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 11.24841435599666\n",
      "-------------------\n",
      "\n",
      "Running epoch 5 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 10.522326436076131.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 9.72009142654202\n",
      "-------------------\n",
      "\n",
      "Running epoch 6 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 9.169493901979672.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 8.452587624861732\n",
      "-------------------\n",
      "\n",
      "Running epoch 7 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 8.05292275568822.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 7.4334790514543725\n",
      "-------------------\n",
      "\n",
      "Running epoch 8 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 7.152782953702486.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 6.621052656128509\n",
      "-------------------\n",
      "\n",
      "Running epoch 9 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 6.4348565915247775.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 5.983951862389443\n",
      "-------------------\n",
      "\n",
      "Running epoch 10 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 5.864399843282633.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 5.487570416870841\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_ctc(cpc_model,character_classifier,loss_ctc,data_loader_train_letters,data_loader_val_letters,optimizer_frozen,n_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "A8oxFr1jm17P",
    "outputId": "947ea6da-7fd9-4106-e3f1-32cf6d77c4f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 27) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the PER computation through beam search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (27 of 27) |########################| Elapsed Time: 0:17:48 Time:  0:17:48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PER 0.9113886992183796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9113886992183796"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_per(data_loader_val_letters,cpc_model,character_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
