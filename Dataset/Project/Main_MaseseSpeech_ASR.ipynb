{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
      "Requirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.5.1+cu101)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchvision) (0.16.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Requirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.1+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchaudio) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchaudio) (1.18.5)\n",
      "Requirement already satisfied: ipdb in /usr/local/lib/python3.6/dist-packages (0.13.3)\n",
      "Requirement already satisfied: ipython>=5.1.0; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (47.3.1)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.8.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.0.18)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.3.3)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (2.1.3)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.12.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Requirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.1+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchaudio) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchaudio) (1.18.5)\n",
      "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
      "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (47.3.1)\n",
      "Collecting soundfile\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.10.3.post1\n"
     ]
    }
   ],
   "source": [
    "# If you need some install, uncomment the code bellow \n",
    "\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install torchaudio\n",
    "!pip install ipdb\n",
    "\n",
    "!pip install torchaudio\n",
    "!pip install PyDrive\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import ipdb\n",
    "\n",
    "# from torchaudio.datasets import YESNO, LIBRISPEECH\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pdb, traceback, sys\n",
    "\n",
    "\n",
    "from torchaudio.datasets.utils import (\n",
    "  download_url,\n",
    "  extract_archive,\n",
    "  walk_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaseseSpeech 2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train link Drive\n",
    "# https://drive.google.com/file/d/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR/view?usp=sharing\n",
    "    \n",
    "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\" -O train-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "    \n",
    "# # Valid link Drive\n",
    "# https://drive.google.com/file/d/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP/view?usp=sharing\n",
    "# https://drive.google.com/file/d/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP/view?usp=sharing\n",
    "    \n",
    "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\" -O dev-clean.tar.xz && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘MaseseSpeech’: File exists\n",
      "--2020-07-03 14:28:50--  https://docs.google.com/uc?export=download&confirm=LWaW&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.214.100, 172.217.214.138, 172.217.214.101, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.214.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-10-6k-docs.googleusercontent.com/docs/securesc/m4ngb04bgeudpldr5p0uoccivec588bs/139749qo4go63r4v5hjtk7q5cnlba4ne/1593786525000/00743790830645195323/04021074871648771280Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e=download [following]\n",
      "--2020-07-03 14:28:50--  https://doc-10-6k-docs.googleusercontent.com/docs/securesc/m4ngb04bgeudpldr5p0uoccivec588bs/139749qo4go63r4v5hjtk7q5cnlba4ne/1593786525000/00743790830645195323/04021074871648771280Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e=download\n",
      "Resolving doc-10-6k-docs.googleusercontent.com (doc-10-6k-docs.googleusercontent.com)... 173.194.192.132, 2607:f8b0:4001:c0e::84\n",
      "Connecting to doc-10-6k-docs.googleusercontent.com (doc-10-6k-docs.googleusercontent.com)|173.194.192.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=qbjjfs047u5ag&continue=https://doc-10-6k-docs.googleusercontent.com/docs/securesc/m4ngb04bgeudpldr5p0uoccivec588bs/139749qo4go63r4v5hjtk7q5cnlba4ne/1593786525000/00743790830645195323/04021074871648771280Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e%3Ddownload&hash=46uqccdpa39g5iph62k8rjsm6kabl0ok [following]\n",
      "--2020-07-03 14:28:50--  https://docs.google.com/nonceSigner?nonce=qbjjfs047u5ag&continue=https://doc-10-6k-docs.googleusercontent.com/docs/securesc/m4ngb04bgeudpldr5p0uoccivec588bs/139749qo4go63r4v5hjtk7q5cnlba4ne/1593786525000/00743790830645195323/04021074871648771280Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e%3Ddownload&hash=46uqccdpa39g5iph62k8rjsm6kabl0ok\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.214.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-10-6k-docs.googleusercontent.com/docs/securesc/m4ngb04bgeudpldr5p0uoccivec588bs/139749qo4go63r4v5hjtk7q5cnlba4ne/1593786525000/00743790830645195323/04021074871648771280Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e=download&nonce=qbjjfs047u5ag&user=04021074871648771280Z&hash=aos6lmt1g0t9r5fnd0og33ll7qp5qbj1 [following]\n",
      "--2020-07-03 14:28:50--  https://doc-10-6k-docs.googleusercontent.com/docs/securesc/m4ngb04bgeudpldr5p0uoccivec588bs/139749qo4go63r4v5hjtk7q5cnlba4ne/1593786525000/00743790830645195323/04021074871648771280Z/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR?e=download&nonce=qbjjfs047u5ag&user=04021074871648771280Z&hash=aos6lmt1g0t9r5fnd0og33ll7qp5qbj1\n",
      "Connecting to doc-10-6k-docs.googleusercontent.com (doc-10-6k-docs.googleusercontent.com)|173.194.192.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-xz]\n",
      "Saving to: ‘MaseseSpeech/train-clean.tar.xz’\n",
      "\n",
      "MaseseSpeech/train-     [     <=>            ] 124.63M   108MB/s    in 1.2s    \n",
      "\n",
      "2020-07-03 14:28:52 (108 MB/s) - ‘MaseseSpeech/train-clean.tar.xz’ saved [130680092]\n",
      "\n",
      "--2020-07-03 14:29:17--  https://docs.google.com/uc?export=download&confirm=&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\n",
      "Resolving docs.google.com (docs.google.com)... 173.194.74.100, 173.194.74.139, 173.194.74.102, ...\n",
      "Connecting to docs.google.com (docs.google.com)|173.194.74.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0c-50-docs.googleusercontent.com/docs/securesc/8v5gr119m7dtcvocm8c2nporb46sgmqp/gdt108rd4f8jft41emfj82k5ll3kl3l7/1593786525000/00743790830645195323/14071733097205781773Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e=download [following]\n",
      "--2020-07-03 14:29:27--  https://doc-0c-50-docs.googleusercontent.com/docs/securesc/8v5gr119m7dtcvocm8c2nporb46sgmqp/gdt108rd4f8jft41emfj82k5ll3kl3l7/1593786525000/00743790830645195323/14071733097205781773Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e=download\n",
      "Resolving doc-0c-50-docs.googleusercontent.com (doc-0c-50-docs.googleusercontent.com)... 173.194.192.132, 2607:f8b0:4001:c0e::84\n",
      "Connecting to doc-0c-50-docs.googleusercontent.com (doc-0c-50-docs.googleusercontent.com)|173.194.192.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=22ufv591c9t6u&continue=https://doc-0c-50-docs.googleusercontent.com/docs/securesc/8v5gr119m7dtcvocm8c2nporb46sgmqp/gdt108rd4f8jft41emfj82k5ll3kl3l7/1593786525000/00743790830645195323/14071733097205781773Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e%3Ddownload&hash=9rb38runqa0rjjc6p531tpja5bak62b0 [following]\n",
      "--2020-07-03 14:29:27--  https://docs.google.com/nonceSigner?nonce=22ufv591c9t6u&continue=https://doc-0c-50-docs.googleusercontent.com/docs/securesc/8v5gr119m7dtcvocm8c2nporb46sgmqp/gdt108rd4f8jft41emfj82k5ll3kl3l7/1593786525000/00743790830645195323/14071733097205781773Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e%3Ddownload&hash=9rb38runqa0rjjc6p531tpja5bak62b0\n",
      "Connecting to docs.google.com (docs.google.com)|173.194.74.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-0c-50-docs.googleusercontent.com/docs/securesc/8v5gr119m7dtcvocm8c2nporb46sgmqp/gdt108rd4f8jft41emfj82k5ll3kl3l7/1593786525000/00743790830645195323/14071733097205781773Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e=download&nonce=22ufv591c9t6u&user=14071733097205781773Z&hash=gq9n6fko3a88j4mj43n0qij27aupbc2i [following]\n",
      "--2020-07-03 14:29:27--  https://doc-0c-50-docs.googleusercontent.com/docs/securesc/8v5gr119m7dtcvocm8c2nporb46sgmqp/gdt108rd4f8jft41emfj82k5ll3kl3l7/1593786525000/00743790830645195323/14071733097205781773Z/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP?e=download&nonce=22ufv591c9t6u&user=14071733097205781773Z&hash=gq9n6fko3a88j4mj43n0qij27aupbc2i\n",
      "Connecting to doc-0c-50-docs.googleusercontent.com (doc-0c-50-docs.googleusercontent.com)|173.194.192.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-xz]\n",
      "Saving to: ‘MaseseSpeech/dev-clean.tar.xz’\n",
      "\n",
      "MaseseSpeech/dev-cl     [  <=>               ]  89.37M   340MB/s    in 0.3s    \n",
      "\n",
      "2020-07-03 14:29:33 (340 MB/s) - ‘MaseseSpeech/dev-clean.tar.xz’ saved [93709260]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir MaseseSpeech\n",
    "!mkdir MaseseSpeech\n",
    "\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\" -O MaseseSpeech/train-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\" -O MaseseSpeech/dev-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "extract_archive(\"MaseseSpeech/train-clean.tar.xz\")\n",
    "extract_archive(\"MaseseSpeech/dev-clean.tar.xz\")\n",
    "\n",
    "!rm -r \"MaseseSpeech/train-clean.tar.xz\"\n",
    "!rm -r \"MaseseSpeech/dev-clean.tar.xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "URL = \"train-clean\"\n",
    "FOLDER_IN_ARCHIVE = \"MaseseSpeech\"\n",
    "# BASE_URL = \"https://dl.fbaipublicfiles.com/librispeech_100h_mp3/\"\n",
    "# _CHECKSUMS = {\n",
    "#   BASE_URL + \"dev-clean.tar.gz\":\n",
    "#   \"076916a8f9c61951c5d2e6efaa8d2188232fcf860eec8c074e46edf4fac9623e\",\n",
    "#   BASE_URL + \"test-clean.tar.gz\":\n",
    "#   \"3c171e2f1e377e4993c2dbe6bff3f01cd324c0ed462f4de6c78737402a7dbedd\",\n",
    "#   BASE_URL + \"train-clean-100.tar.gz\":\n",
    "#   \"7bfbefc680d25ba3a82798ce32c287ea0e82932af1b1f864fae71fb52d2f41f0\",\n",
    "# }\n",
    "\n",
    "\n",
    "def load_masesespeech_item(fileid: str, \n",
    "                          path: str, \n",
    "                          ext_audio: str, \n",
    "                          ext_txt: str) -> Tuple[Tensor, int, str, int, int, int]:\n",
    "    \n",
    "    speaker_id, chapter_id, utterance_id = fileid.split(\"-\")\n",
    "    \n",
    "    file_text = speaker_id + \"-\" + chapter_id + ext_txt\n",
    "    file_text = os.path.join(path, speaker_id, chapter_id, file_text)\n",
    "    \n",
    "    fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\n",
    "    file_audio = fileid_audio + ext_audio\n",
    "    file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    try :\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(file_audio)\n",
    "\n",
    "        # Load text\n",
    "        with open(file_text) as ft:\n",
    "            for line in ft:\n",
    "                fileid_text, utterance = line.strip().split(\" \", 1)\n",
    "                if fileid_audio == fileid_text:\n",
    "                    break\n",
    "            else:\n",
    "              # Translation not found\n",
    "              raise FileNotFoundError(\"Translation not found for \" + fileid_audio)\n",
    "    except:\n",
    "        print(file_audio)\n",
    "        print(waveform)\n",
    "        pass\n",
    "#         traceback.print_exc()\n",
    "                \n",
    "    return (\n",
    "        waveform,\n",
    "        sample_rate,\n",
    "        utterance,\n",
    "#         int(speaker_id),\n",
    "        int(chapter_id),\n",
    "        int(utterance_id)\n",
    "        )\n",
    "\n",
    "\n",
    "class MASESESPEECH_2H_MP3(Dataset):\n",
    "    \"\"\"\n",
    "    Create a Dataset for MaseseSpeech. Each item is a tuple of the form:\n",
    "    waveform, utterance, chapter_id, verse_id, utterance_id\n",
    "    \"\"\"\n",
    "    \n",
    "    _ext_txt = \".trans.txt\"\n",
    "    _ext_audio = \".wav\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 mode: str = \"MaseseSpeech/train-clean\",\n",
    "                 folder_in_archive: str = FOLDER_IN_ARCHIVE,\n",
    "#                  download: bool = False\n",
    "                ) -> None:\n",
    "#         if url in [\n",
    "#             \"dev-clean\",\n",
    "# #             \"test-clean\",\n",
    "#             \"train-clean\",]:\n",
    "            \n",
    "#             ext_archive = \".tar.xz\"\n",
    "#             base_url = BASE_URL\n",
    "#             url = os.path.join(base_url, url + ext_archive)\n",
    "\n",
    "#         basename = os.path.basename(url)\n",
    "#         archive = os.path.join(root, basename)\n",
    "\n",
    "#         basename = basename.split(\".\")[0]\n",
    "#         folder_in_archive = os.path.join(folder_in_archive, basename)\n",
    "        \n",
    "        \n",
    "        self._path = mode\n",
    "\n",
    "#         if download:\n",
    "#             if not os.path.isdir(self._path):\n",
    "#                 if not os.path.isfile(archive):\n",
    "#                     checksum = _CHECKSUMS.get(url, None)\n",
    "#                     download_url(url, root, hash_value=checksum)\n",
    "        \n",
    "        walker = walk_files(\n",
    "          self._path, suffix=self._ext_audio, prefix=False, remove_suffix=True\n",
    "        )\n",
    "        self._walker = list(walker)\n",
    "    def __getitem__(self, n: int) -> Tuple[Tensor, int, str, int, int, int]:\n",
    "        fileid = self._walker[n]\n",
    "        return load_masesespeech_item(fileid, self._path, self._ext_audio, self._ext_txt)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._walker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "masese_train = MASESESPEECH_2H_MP3(\".\", mode = \"MaseseSpeech/train-clean\")\n",
    "masese_dev = MASESESPEECH_2H_MP3(\".\", mode = \"MaseseSpeech/dev-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0923, -0.0903, -0.0939]]), 16000, 'NA LOFUNDO, MOTO ABIMISAKA KAKA ETUMBA,  KASI BWANYA EZALI EPAI YA BAOYO BAPESANAKA MAKANISI.', 13, 9)\n",
      "(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0609, 0.0486, 0.0518]]), 16000, 'MPO LITOMBA YA KOZWA YANGO ELEKI LITOMBA YA KOZWA PALATA MPE KOZWA YANGO ELEKI KOZWA WOLO.', 3, 13)\n"
     ]
    }
   ],
   "source": [
    "# just so you get an idea of the format \n",
    "print(next(iter(masese_train)))\n",
    "print(next(iter(masese_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Ɛ', ',', ';', '.', 'Ɔ', 'Á', 'Ó', '-', '?', ':', '*', 'É', '—', '“', '”', '!', 'Í', '(', '1', '0', ')']\n"
     ]
    }
   ],
   "source": [
    "# Use this is your acoustic model is outputting letters\n",
    "special_caracters = \"Ɛ,;.ƆÁÓ-?:*É—“”!Í(10)\"\n",
    "tokens_list = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ\"+special_caracters)\n",
    "tokens_set = set(tokens_list)\n",
    "print(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_libri(batch):\n",
    "    #print(batch)\n",
    "    tensors = [b[0].t() for b in batch if b]\n",
    "    tensors_len = [len(t) for t in tensors]\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "    \n",
    "    transcriptions = [list(b[2].replace(\"'\", \" \")) for b in batch if b]\n",
    "    targets = [torch.tensor([tokens_list.index(e) for e in t]) for t in transcriptions]\n",
    "    targets_len = [len(t) for t in targets]\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "    return tensors, targets, torch.tensor(tensors_len), torch.tensor(targets_len)\n",
    "\n",
    "# train_set = torch.utils.data.DataLoader(masese_train, batch_size=900, shuffle=True,\n",
    "#                                         num_workers=4, collate_fn=collate_fn_libri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchaudio.load(\"MaseseSpeech/train-clean/020/008/020-008-017.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(train_set)))\n",
    "train_set = torch.utils.data.DataLoader(masese_train, batch_size=64, shuffle=True,\n",
    "                                        num_workers=2, collate_fn=collate_fn_libri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_libri(batch):\n",
    "    #print(batch)\n",
    "    tensors = [b[0].t() for b in batch if b]\n",
    "    tensors_len = [len(t) for t in tensors]\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "    \n",
    "    transcriptions = [list(b[2].replace(\"'\", \" \")) for b in batch if b]\n",
    "    targets = [torch.tensor([tokens_list.index(e) for e in t]) for t in transcriptions]\n",
    "    targets_len = [len(t) for t in targets]\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "    return tensors, targets, torch.tensor(tensors_len), torch.tensor(targets_len)\n",
    "\n",
    "valid_set = torch.utils.data.DataLoader(masese_dev, batch_size=1500, shuffle=True,\n",
    "                                        num_workers=4, collate_fn=collate_fn_libri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[11, 15, 19,  ...,  0,  0,  0],\n",
      "        [14,  1,  0,  ...,  0,  0,  0],\n",
      "        [14,  1,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 2, 15, 12,  ...,  0,  0,  0],\n",
      "        [13, 16, 15,  ...,  0,  0,  0],\n",
      "        [13, 23,  1,  ...,  0,  0,  0]]), tensor([183315, 163350, 350658, 288585, 224697, 193479, 188760, 169884, 157542,\n",
      "        253737, 162261, 116160, 176418, 235950, 142296, 120153, 185493, 293304,\n",
      "        132495, 233046, 238150, 135762, 186219, 182952, 184404, 225060, 186582,\n",
      "        223245, 267894, 204732, 249381, 161535, 173151, 246114, 217437, 263538,\n",
      "        178959, 202917, 198924, 224697, 207636, 223971, 197109, 146289, 280236,\n",
      "        117975, 217437, 191301, 170610, 221793, 104181, 172062, 233046, 200376,\n",
      "        342309, 217437, 178233, 172062, 253737, 171336, 206184, 202191, 129228,\n",
      "        214533, 167728, 181863, 201828, 221430, 188397, 235224, 225060, 284592,\n",
      "        176418, 147400, 195657, 116886, 172788, 181522, 281325, 158631, 276606,\n",
      "        151371, 233409, 198561, 265716, 140118, 160809, 246840, 239217, 149919,\n",
      "        138303, 150282, 151371, 186219, 196383, 152823, 228690, 199650, 181500,\n",
      "        197472, 233409, 160809, 222519, 160083, 205095, 167343, 168069, 126687,\n",
      "        208021, 249744, 136851, 222882, 170610, 235950, 175329, 232320, 185493,\n",
      "        207636, 193842, 129228, 173877, 212718, 213807, 221067, 152097, 274428,\n",
      "        181863, 184767, 164076, 242484, 166980, 216348, 268620, 198924,  87120,\n",
      "        196383, 126687, 242121, 192753, 211992, 133221, 196746, 232320, 143385,\n",
      "        163350, 190575, 158268, 199287, 178233, 257004, 225786, 220000, 119064,\n",
      "        232320, 169158, 234861, 222882, 155001, 140844, 195657, 191664, 120153,\n",
      "        161898, 118338, 120153, 212355, 168795, 247929, 203280, 144837, 183315,\n",
      "        180774, 196383, 152460, 117634, 160446, 145926, 141570, 180411, 248292,\n",
      "        178596, 190938, 175692, 194568, 152460, 164076, 182589, 161194, 191664,\n",
      "        158290, 235972, 162987, 163350, 169158, 276606, 209814, 121968, 115797,\n",
      "        116886, 193842, 211266, 252285, 204732, 163350, 185130, 197109, 152460,\n",
      "        209451, 125961, 216348, 255189, 147015, 178233, 169884, 345939, 181137,\n",
      "        152097, 211266, 196020, 137214, 165891, 301290, 237039, 128865, 180411,\n",
      "        196746, 266079, 161172, 203643, 188034, 148104, 174625, 217074, 155364,\n",
      "        210903, 211992, 197109, 197109, 172425, 193116, 161535, 206547, 145926,\n",
      "        176418, 183315, 163713, 161172, 227964, 215259, 186582, 140118, 169521,\n",
      "        254826, 130680, 295119, 238128, 141207, 164076, 232320, 166980, 211629,\n",
      "        179322, 123783, 150282, 198198, 356829, 145926, 159357, 273339, 165165,\n",
      "        178959, 151008, 223245, 168069, 171699]), tensor([118,  92, 163, 109, 115,  98,  83,  68,  91,  85,  87,  71,  78, 117,\n",
      "         93,  83, 108, 111,  81, 111, 117,  69, 111, 108, 105, 129,  79,  90,\n",
      "        124, 103, 126,  85,  78, 135, 106, 165,  86, 130,  99, 124,  85, 100,\n",
      "        105,  70,  96,  63, 148, 101, 105, 104,  77,  57, 129, 110, 133, 107,\n",
      "         80,  86, 110,  79,  98, 117,  90,  84, 106, 104, 108,  79, 107, 108,\n",
      "         98, 105, 106,  71, 116,  94,  83, 105, 175,  62, 120,  86, 119, 115,\n",
      "        127,  89,  78,  92, 131,  75,  81,  80,  86, 102, 107,  91, 100,  90,\n",
      "        120,  84,  93, 100, 130,  72,  95, 104,  84,  66,  92, 130,  78,  96,\n",
      "        111, 110,  79, 108,  97, 107, 106,  83, 115, 108, 117,  95,  88, 120,\n",
      "         88,  94,  87,  97,  94, 140, 133, 104,  69,  90,  73, 117,  78, 115,\n",
      "         59,  88, 130,  83, 100, 118,  95,  89,  76, 123, 139, 108,  74, 116,\n",
      "         86,  92, 101,  92,  95, 109, 103,  78,  72,  70,  70, 116,  90, 120,\n",
      "        115,  78,  93,  95,  80,  79,  79,  75,  83,  92,  90,  89, 100, 101,\n",
      "         97, 119,  65,  87, 100,  84,  88,  89, 138,  96,  87,  97, 104, 102,\n",
      "         63,  62,  62,  99, 111, 110,  97, 102, 105,  92,  87, 103,  73, 115,\n",
      "         99,  94, 105,  81, 139,  96,  82, 102, 105,  63,  81, 114, 112,  71,\n",
      "         93, 130, 123, 100,  95,  94,  88, 103,  72,  78, 106, 108, 111, 117,\n",
      "        105,  94,  84, 107,  78,  85,  95,  93, 102,  93,  92, 102,  92,  96,\n",
      "        120,  58, 143, 118,  94,  71, 144, 102, 147,  74,  62,  99,  92, 143,\n",
      "         87,  68, 122,  86, 115,  88, 105,  69,  98]))\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(valid_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CPC_audio'...\n",
      "remote: Enumerating objects: 84, done.\u001b[K\n",
      "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "remote: Total 84 (delta 13), reused 75 (delta 6), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (84/84), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/facebookresearch/CPC_audio.git  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/AMMI-Speech-Lig-Aikuma/Dataset/Project/CPC_audio\n"
     ]
    }
   ],
   "source": [
    "%cd CPC_audio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE_OF_CONDUCT.md  CPC_audio.egg-info\tREADME.md  cpc\t\t    hubconf.py\n",
      "CONTRIBUTING.md     LICENSE\t\tbuild\t   environment.yml  setup.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running develop\n",
      "running egg_info\n",
      "writing CPC_audio.egg-info/PKG-INFO\n",
      "writing dependency_links to CPC_audio.egg-info/dependency_links.txt\n",
      "writing top-level names to CPC_audio.egg-info/top_level.txt\n",
      "writing manifest file 'CPC_audio.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "copying build/lib.linux-x86_64-3.6/cpc/eval/ABX/dtw.cpython-36m-x86_64-linux-gnu.so -> cpc/eval/ABX\n",
      "Creating /usr/local/lib/python3.6/dist-packages/CPC-audio.egg-link (link to .)\n",
      "CPC-audio 1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /root/AMMI-Speech-Lig-Aikuma/Dataset/Project/CPC_audio\n",
      "Processing dependencies for CPC-audio==1.0\n",
      "Finished processing dependencies for CPC-audio==1.0\n"
     ]
    }
   ],
   "source": [
    "# !%cd /CPC_audio\n",
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Building the model\n",
    "\n",
    "In this exercise, we will build and train a small CPC model using the repository CPC_audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/CPC_audio\n",
    "from cpc.model import CPCEncoder, CPCAR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DIM_ENCODER=256\n",
    "DIM_CONTEXT=256\n",
    "KEEP_HIDDEN_VECTOR=False\n",
    "N_LEVELS_CONTEXT=1\n",
    "CONTEXT_RNN=\"LSTM\"\n",
    "N_PREDICTIONS=12\n",
    "LEARNING_RATE=2e-4\n",
    "N_NEGATIVE_SAMPLE =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CPCEncoder(DIM_ENCODER)\n",
    "context = CPCAR(DIM_ENCODER,\n",
    "                DIM_CONTEXT,\n",
    "                KEEP_HIDDEN_VECTOR,\n",
    "                N_LEVELS_CONTEXT,\n",
    "                mode=\"CONTEXT_RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several functions that will be necessary to load the data later\n",
    "from cpc.dataset import findAllSeqs, AudioBatchData, parseSeqLabels\n",
    "SIZE_WINDOW = 20480\n",
    "BATCH_SIZE=8\n",
    "def load_dataset(path_dataset, file_extension='.wav', phone_label_dict=None):\n",
    "    data_list, speakers = findAllSeqs(path_dataset, extension=file_extension)\n",
    "    dataset = AudioBatchData(path_dataset, SIZE_WINDOW, data_list, phone_label_dict, len(speakers))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPCModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 AR):\n",
    "\n",
    "        super(CPCModel, self).__init__()\n",
    "        self.gEncoder = encoder\n",
    "        self.gAR = AR\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        \n",
    "\n",
    "        encoder_output = self.gEncoder(batch_data)\n",
    "        #print(encoder_output.shape)\n",
    "        # The output of the encoder data does not have the good format \n",
    "        # indeed it is Batch_size x Hidden_size x temp size\n",
    "        # while the context requires Batch_size  x temp size x Hidden_size\n",
    "        # thus you need to permute\n",
    "        context_input = encoder_output.permute(0, 2, 1)\n",
    "\n",
    "        context_output = self.gAR(context_input)\n",
    "        #print(context_output.shape)\n",
    "        return context_output, encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torchaudio.load(\n",
    "        \"../MaseseSpeech/train-clean/020/001/020-001-013.wav\")[0]\n",
    "\n",
    "audio = audio.view(1, 1, -1)\n",
    "\n",
    "cpc_model = CPCModel(encoder, context).to(device)\n",
    "context_output, encoder_output = cpc_model(audio.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.1118, 0.0728, 0.0479]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 : CPC loss\n",
    "\n",
    "We will define a class ```CPCCriterion``` which will hold the prediction networks $\\phi_k$ defined above and perform the classification loss $\\mathcal{L}_c$.\n",
    "\n",
    "a) In this exercise, the $\\phi_k$ will be a linear transform, ie:\n",
    "\n",
    "\\\\[ \\phi_k(c_t) = \\mathbf{A}_k c_t\\\\]\n",
    "\n",
    "Using the class [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear), define the transformations $\\phi_k$ in the code below and complete the function ```get_prediction_k``` which computes $\\phi_k(c_t)$ for a given batch of vectors $c_t$.\n",
    "\n",
    "b) Using both ```get_prediction_k```  and ```sample_negatives``` defined below, write the forward function which will take as input two batches of features $c_t$ and $g_t$ and outputs the classification loss $\\mathcal{L}_c$ and the average acuracy for all predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2: write the CPC loss\n",
    "# a) Write the negative sampling (with some help)\n",
    "# ERRATUM: it's really hard, the sampling will be provided\n",
    "\n",
    "class CPCCriterion(torch.nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               K,\n",
    "               dim_context,\n",
    "               dim_encoder,\n",
    "               n_negative):\n",
    "    super(CPCCriterion, self).__init__()\n",
    "    self.K_ = K\n",
    "    self.dim_context = dim_context\n",
    "    self.dim_encoder = dim_encoder\n",
    "    self.n_negative = n_negative\n",
    "\n",
    "    self.predictors = torch.nn.ModuleList() \n",
    "    for k in range(self.K_):\n",
    "      # TO COMPLETE !\n",
    "      \n",
    "      # A affine transformation in pytorch is equivalent to a nn.Linear layer\n",
    "      # To get a linear transformation you must set bias=False\n",
    "      # input dimension of the layer = dimension of the encoder\n",
    "      # output dimension of the layer = dimension of the context\n",
    "      self.predictors.append(torch.nn.Linear(dim_context, dim_encoder, bias=False))\n",
    "\n",
    "  def get_prediction_k(self, context_data):\n",
    "\n",
    "    #TO COMPLETE !\n",
    "    output = [] \n",
    "    # For each time step k\n",
    "    for k in range(self.K_):\n",
    "\n",
    "      # We need to compute phi_k = A_k * c_t\n",
    "      phi_k = self.predictors[k](context_data)\n",
    "      output.append(phi_k)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "  def sample_negatives(self, encoded_data):\n",
    "    r\"\"\"\n",
    "    Sample some negative examples in the given encoded data.\n",
    "    Input:\n",
    "    - encoded_data size: B x T x H\n",
    "    Returns\n",
    "    - outputs of size B x (n_negative + 1) x (T - K_) x H\n",
    "      outputs[:, 0, :, :] contains the positive example\n",
    "      outputs[:, 1:, :, :] contains negative example sampled in the batch\n",
    "    - labels, long tensor of size B x (T - K_)\n",
    "      Since the positive example is always at coordinates 0 for all sequences \n",
    "      in the batch and all timestep in the sequence, labels is just a tensor\n",
    "      full of zeros !\n",
    "    \"\"\"\n",
    "    batch_size, time_size, dim_encoded = encoded_data.size()\n",
    "    window_size = time_size - self.K_\n",
    "    outputs = []\n",
    "\n",
    "    neg_ext = encoded_data.contiguous().view(-1, dim_encoded)\n",
    "    n_elem_sampled = self.n_negative * window_size * batch_size\n",
    "    # Draw nNegativeExt * batchSize negative samples anywhere in the batch\n",
    "    batch_idx = torch.randint(low=0, high=batch_size,\n",
    "                              size=(n_elem_sampled, ),\n",
    "                              device=encoded_data.device)\n",
    "\n",
    "    seq_idx = torch.randint(low=1, high=time_size,\n",
    "                            size=(n_elem_sampled, ),\n",
    "                            device=encoded_data.device)\n",
    "\n",
    "    base_idx = torch.arange(0, window_size, device=encoded_data.device)\n",
    "    base_idx = base_idx.view(1, 1, window_size)\n",
    "    base_idx = base_idx.expand(1, self.n_negative, window_size)\n",
    "    base_idx = base_idx.expand(batch_size, self.n_negative, window_size)\n",
    "    seq_idx += base_idx.contiguous().view(-1)\n",
    "    seq_idx = torch.remainder(seq_idx, time_size)\n",
    "\n",
    "    ext_idx = seq_idx + batch_idx * time_size\n",
    "    neg_ext = neg_ext[ext_idx].view(batch_size, self.n_negative,\n",
    "                                    window_size, dim_encoded)\n",
    "    label_loss = torch.zeros((batch_size, window_size),\n",
    "                              dtype=torch.long,\n",
    "                              device=encoded_data.device)\n",
    "\n",
    "    for k in range(1, self.K_ + 1):\n",
    "\n",
    "      # Positive samples\n",
    "      if k < self.K_:\n",
    "          pos_seq = encoded_data[:, k:-(self.K_-k)]\n",
    "      else:\n",
    "          pos_seq = encoded_data[:, k:]\n",
    "\n",
    "      pos_seq = pos_seq.view(batch_size, 1, pos_seq.size(1), dim_encoded)\n",
    "      full_seq = torch.cat((pos_seq, neg_ext), dim=1)\n",
    "      outputs.append(full_seq)\n",
    "\n",
    "    return outputs, label_loss\n",
    "\n",
    "  def forward(self, encoded_data, context_data):\n",
    "\n",
    "    # TO COMPLETE:\n",
    "    # Perform the full cpc criterion\n",
    "    # Returns 2 values:\n",
    "    # - the average classification loss avg_loss\n",
    "    # - the average classification acuracy avg_acc\n",
    "\n",
    "    # Reminder : The permuation !\n",
    "    encoded_data = encoded_data.permute(0, 2, 1)\n",
    "\n",
    "    # First we need to sample the negative examples\n",
    "    negative_samples, labels = self.sample_negatives(encoded_data)\n",
    "\n",
    "    # Then we must compute phi_k\n",
    "    phi_k = self.get_prediction_k(context_data)\n",
    "\n",
    "    # Finally we must get the dot product between phi_k and negative_samples \n",
    "    # for each k\n",
    "\n",
    "    #The total loss is the average of all losses\n",
    "    avg_loss = 0\n",
    "\n",
    "    # Average acuracy\n",
    "    avg_acc = 0\n",
    "\n",
    "    for k in range(self.K_):\n",
    "      B, N_sampled, S_small, H = negative_samples[k].size() \n",
    "      B, S, H = phi_k[k].size()\n",
    "\n",
    "      # As told before S = S_small + K. For segments too far in the sequence\n",
    "      # there are no positive exmples anyway, so we must shorten phi_k\n",
    "      phi = phi_k[k][:, :S_small]\n",
    "\n",
    "      # Now the dot product\n",
    "      # You have several ways to do that, let's do the simple but non optimal \n",
    "      # one\n",
    "      # pytorch has a matrix product function https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "      # But it takes only 3D tensors of the same batch size !\n",
    "      # To begin negative_samples is a 4D tensor ! \n",
    "      # We want to compute the dot product for each features, of each sequence\n",
    "      # of the batch. Thus we are trying to compute a dot product for all\n",
    "      # B* N_sampled * S_small 1D vector of negative_samples[k]\n",
    "      # Or, a 1D tensor of size H is also a matrix of size 1 x H\n",
    "      # Then, we must view it as a 3D tensor of size (B* N_sampled * S_small, 1, H)\n",
    "      negative_sample_k  =  negative_samples[k].view(B* N_sampled* S_small, 1, H)\n",
    "\n",
    "      # But now phi and negative_sample_k no longer have the same batch size !\n",
    "      # No worries, we can expand phi so that each sequence of the batch\n",
    "      # is repeated N_sampled times\n",
    "      phi = phi.view(B, 1,S_small, H).expand(B, N_sampled, S_small, H)\n",
    "\n",
    "      # And now we can view it as a 3D tensor \n",
    "      phi  = phi.contiguous().view(B * N_sampled * S_small, H, 1)\n",
    "\n",
    "      # We can finally get the dot product !\n",
    "      scores = torch.bmm(negative_sample_k, phi)\n",
    "\n",
    "      # Dot_product has a size (B * N_sampled * S_small , 1, 1)\n",
    "      # Let's reorder it a bit\n",
    "      scores = scores.reshape(B, N_sampled, S_small)\n",
    "\n",
    "      # For each elements of the sequence, and each elements sampled, it gives \n",
    "      # a floating score stating the likelihood of this element being the \n",
    "      # true one.\n",
    "      # Now the classification loss, we need to use the Cross Entropy loss\n",
    "      # https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "      # For each time-step of each sequence of the batch \n",
    "      # we have N_sampled possible predictions. \n",
    "      # Looking at the documentation of torch.nn.CrossEntropyLoss\n",
    "      # we can see that this loss expect a tensor of size M x C where \n",
    "      # - M is the number of elements with a classification score\n",
    "      # - C is the number of possible classes\n",
    "      # There are N_sampled candidates for each predictions so\n",
    "      # C = N_sampled \n",
    "      # Each timestep of each sequence of the batch has a prediction so\n",
    "      # M = B * S_small\n",
    "      # Thus we need an input vector of size B * S_small, N_sampled\n",
    "      # To begin, we need to permute the axis\n",
    "      scores = scores.permute(0, 2, 1) # Now it has size B , S_small, N_sampled\n",
    "\n",
    "      # Then we can cast it into a 2D tensor\n",
    "      scores = scores.reshape(B * S_small, N_sampled)\n",
    "\n",
    "      # Same thing for the labels \n",
    "      labels = labels.reshape(B * S_small)\n",
    "\n",
    "      # Finally we can get the classification loss\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "      loss_k = loss_criterion(scores, labels)\n",
    "      avg_loss+= loss_k\n",
    "\n",
    "      # And for the acuracy\n",
    "      # The prediction for each elements is the sample with the highest score\n",
    "      # Thus the tensors of all predictions is the tensors of the index of the \n",
    "      # maximal score for each time-step of each sequence of the batch\n",
    "      predictions = torch.argmax(scores, 1)\n",
    "      acc_k  = (labels == predictions).sum() / (B * S_small)\n",
    "      avg_acc += acc_k\n",
    "\n",
    "    # Normalization\n",
    "    avg_loss = avg_loss / self.K_\n",
    "    avg_acc = avg_acc / self.K_\n",
    "      \n",
    "    return avg_loss , avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "audio = torchaudio.load(\n",
    "    \"../MaseseSpeech/train-clean/020/001/020-001-013.wav\")[0]\n",
    "\n",
    "audio = audio.view(1, 1, -1)\n",
    "cpc_criterion = CPCCriterion(N_PREDICTIONS, DIM_CONTEXT, \n",
    "                             DIM_ENCODER, N_NEGATIVE_SAMPLE).to(device)\n",
    "context_output, encoder_output = cpc_model(audio.to(device))\n",
    "loss, avg = cpc_criterion(encoder_output,context_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3002, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Full training loop !\n",
    "\n",
    "You have the model, you have the criterion. All you need now are a data loader and an optimizer to run your training loop.\n",
    "\n",
    "We will use an Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(cpc_criterion.parameters()) + list(cpc_model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 5599.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache file at ../MaseseSpeech/train-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "363it [00:00, 835637.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking length...\n",
      "Done, elapsed: 0.030 seconds\n",
      "Scanned 363 sequences in 0.03 seconds\n",
      "1 chunks computed\n",
      "Joining pool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_train = load_dataset('../MaseseSpeech/train-clean')\n",
    "dataset_val = load_dataset('../MaseseSpeech/dev-clean')\n",
    "data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n",
    "data_loader_val = dataset_train.getDataLoader(BATCH_SIZE, \"sequence\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
