{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need some install, uncomment the code bellow \n",
    "\n",
    "# !pip install torch==1.4\n",
    "# !pip install torchvision\n",
    "# !pip install ipdb\n",
    "\n",
    "# !pip install torchaudio\n",
    "# !pip install PyDrive\n",
    "# !pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import ipdb\n",
    "\n",
    "# from torchaudio.datasets import YESNO, LIBRISPEECH\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pdb, traceback, sys\n",
    "\n",
    "\n",
    "from torchaudio.datasets.utils import (\n",
    "  download_url,\n",
    "  extract_archive,\n",
    "  walk_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaseseSpeech 2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train link Drive\n",
    "# https://drive.google.com/file/d/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR/view?usp=sharing\n",
    "    \n",
    "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\" -O train-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "    \n",
    "# # Valid link Drive\n",
    "# https://drive.google.com/file/d/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP/view?usp=sharing\n",
    "# https://drive.google.com/file/d/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP/view?usp=sharing\n",
    "    \n",
    "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\" -O dev-clean.tar.xz && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-04 17:46:18--  https://github.com/Kabongosalomon/MaseseSpeech/raw/master/voalingala.tar.xz\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Kabongosalomon/MaseseSpeech/master/voalingala.tar.xz [following]\n",
      "--2020-07-04 17:46:18--  https://raw.githubusercontent.com/Kabongosalomon/MaseseSpeech/master/voalingala.tar.xz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32168212 (31M) [application/octet-stream]\n",
      "Saving to: ‘voalingala.tar.xz’\n",
      "\n",
      "voalingala.tar.xz   100%[===================>]  30.68M  64.3MB/s    in 0.5s    \n",
      "\n",
      "2020-07-04 17:46:20 (64.3 MB/s) - ‘voalingala.tar.xz’ saved [32168212/32168212]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget  https://github.com/Kabongosalomon/MaseseSpeech/raw/master/voalingala.tar.xz -O voalingala.tar.xz\n",
    "\n",
    "extract_archive(\"voalingala.tar.xz\")\n",
    "!rm -r \"voalingala.tar.xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir MaseseSpeech\n",
    "# !mkdir MaseseSpeech\n",
    "\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\" -O MaseseSpeech/train-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\" -O MaseseSpeech/dev-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "# extract_archive(\"MaseseSpeech/train-clean.tar.xz\")\n",
    "# extract_archive(\"MaseseSpeech/dev-clean.tar.xz\")\n",
    "\n",
    "# !rm -r \"MaseseSpeech/train-clean.tar.xz\"\n",
    "# !rm -r \"MaseseSpeech/dev-clean.tar.xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"train-clean\"\n",
    "FOLDER_IN_ARCHIVE = \"MaseseSpeech\"\n",
    "# BASE_URL = \"https://dl.fbaipublicfiles.com/librispeech_100h_mp3/\"\n",
    "# _CHECKSUMS = {\n",
    "#   BASE_URL + \"dev-clean.tar.gz\":\n",
    "#   \"076916a8f9c61951c5d2e6efaa8d2188232fcf860eec8c074e46edf4fac9623e\",\n",
    "#   BASE_URL + \"test-clean.tar.gz\":\n",
    "#   \"3c171e2f1e377e4993c2dbe6bff3f01cd324c0ed462f4de6c78737402a7dbedd\",\n",
    "#   BASE_URL + \"train-clean-100.tar.gz\":\n",
    "#   \"7bfbefc680d25ba3a82798ce32c287ea0e82932af1b1f864fae71fb52d2f41f0\",\n",
    "# }\n",
    "\n",
    "\n",
    "def load_masesespeech_item(fileid: str, \n",
    "                          path: str, \n",
    "                          ext_audio: str, \n",
    "                          ext_txt: str) -> Tuple[Tensor, int, str, int, int, int]:\n",
    "    \n",
    "    book_id, chapter_id, utterance_id = fileid.split(\"-\")\n",
    "    \n",
    "    file_text = book_id + \"-\" + chapter_id + ext_txt\n",
    "    file_text = os.path.join(path, book_id, chapter_id, file_text)\n",
    "    \n",
    "    fileid_audio = book_id + \"-\" + chapter_id + \"-\" + utterance_id\n",
    "    file_audio = fileid_audio + ext_audio\n",
    "    file_audio = os.path.join(path, book_id, chapter_id, file_audio)\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    try :\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(file_audio)\n",
    "\n",
    "        # Load text\n",
    "        with open(file_text) as ft:\n",
    "            for line in ft:\n",
    "                fileid_text, utterance = line.strip().split(\" \", 1) # this takes the first space split\n",
    "                if fileid_audio == fileid_text:\n",
    "                    # stop when we found the text corresponding to \n",
    "                    # the audio ID\n",
    "                    break\n",
    "            else:\n",
    "              # Translation not found\n",
    "              raise FileNotFoundError(\"Translation not found for \" + fileid_audio)\n",
    "    except:\n",
    "        print(file_audio) # this is for debugging purpose \n",
    "        print(waveform)   # to show which file may have an issue \n",
    "        pass\n",
    "#         traceback.print_exc()\n",
    "                \n",
    "    return (\n",
    "        waveform,\n",
    "        sample_rate,\n",
    "        utterance,\n",
    "        int(book_id),\n",
    "        int(chapter_id),\n",
    "        int(utterance_id)\n",
    "        )\n",
    "\n",
    "\n",
    "class MASESESPEECH_2H_MP3(Dataset):\n",
    "    \"\"\"\n",
    "    Create a Dataset for MaseseSpeech. Each item is a tuple of the form:\n",
    "    waveform, utterance, chapter_id, verse_id, utterance_id\n",
    "    \"\"\"\n",
    "    \n",
    "    _ext_txt = \".trans.txt\"\n",
    "    _ext_audio = \".wav\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 mode: str = \"MaseseSpeech/train-clean\",\n",
    "                 folder_in_archive: str = FOLDER_IN_ARCHIVE,\n",
    "#                  download: bool = False\n",
    "                ) -> None:\n",
    "#         if url in [\n",
    "#             \"dev-clean\",\n",
    "# #             \"test-clean\",\n",
    "#             \"train-clean\",]:\n",
    "            \n",
    "#             ext_archive = \".tar.xz\"\n",
    "#             base_url = BASE_URL\n",
    "#             url = os.path.join(base_url, url + ext_archive)\n",
    "\n",
    "#         basename = os.path.basename(url)\n",
    "#         archive = os.path.join(root, basename)\n",
    "\n",
    "#         basename = basename.split(\".\")[0]\n",
    "#         folder_in_archive = os.path.join(folder_in_archive, basename)\n",
    "        \n",
    "        \n",
    "        self._path = mode\n",
    "\n",
    "#         if download:\n",
    "#             if not os.path.isdir(self._path):\n",
    "#                 if not os.path.isfile(archive):\n",
    "#                     checksum = _CHECKSUMS.get(url, None)\n",
    "#                     download_url(url, root, hash_value=checksum)\n",
    "        \n",
    "        walker = walk_files(\n",
    "          self._path, suffix=self._ext_audio, prefix=False, remove_suffix=True\n",
    "        )\n",
    "        self._walker = list(walker)\n",
    "    def __getitem__(self, n: int) -> Tuple[Tensor, int, str, int, int, int]:\n",
    "        fileid = self._walker[n]\n",
    "        return load_masesespeech_item(fileid, self._path, self._ext_audio, self._ext_txt)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._walker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "masese_train = MASESESPEECH_2H_MP3(\".\", mode = \"MaseseSpeech/train-clean\")\n",
    "masese_dev = MASESESPEECH_2H_MP3(\".\", mode = \"MaseseSpeech/dev-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.0149, 0.0132]]), 16000, 'BILOKO YA MOTUYA OYO EUTI NA MAKAMBO YA MPAMBA EKÓMAKA MOKE,  KASI MOTO OYO AYANGANISAKA NA LOBƆKƆ NDE AKOLISAKA YANGO.', 20, 13, 10)\n",
      "(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0237, 0.0286]]), 16000, 'MPO BOLAI YA MIKOLO, BAMBULA YA BOMOI  NÁ KIMYA EKOBAKISAMELA YO.', 20, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "# just so you get an idea of the format \n",
    "print(next(iter(masese_train)))\n",
    "print(next(iter(masese_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Ɛ', ',', ';', '.', 'Ɔ', 'Á', 'Ó', '-', '?', ':', '*', 'É', '—', '“', '”', '!', 'Í', '(', '1', '0', ')']\n"
     ]
    }
   ],
   "source": [
    "# Use this is your acoustic model is outputting letters\n",
    "special_caracters = \"Ɛ,;.ƆÁÓ-?:*É—“”!Í(10)\"    # Special caracters special to Lingala and this dataset\n",
    "tokens_list = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ\"+special_caracters)\n",
    "tokens_set = set(tokens_list)\n",
    "print(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_libri(batch):\n",
    "    #print(batch)\n",
    "    tensors = [b[0].t() for b in batch if b]\n",
    "    tensors_len = [len(t) for t in tensors]\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "    \n",
    "    transcriptions = [list(b[2].replace(\"'\", \" \")) for b in batch if b]\n",
    "    targets = [torch.tensor([tokens_list.index(e) for e in t]) for t in transcriptions]\n",
    "    targets_len = [len(t) for t in targets]\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "    return tensors, targets, torch.tensor(tensors_len), torch.tensor(targets_len)\n",
    "\n",
    "# train_set = torch.utils.data.DataLoader(masese_train, batch_size=900, shuffle=True,\n",
    "#                                         num_workers=4, collate_fn=collate_fn_libri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchaudio.load(\"MaseseSpeech/train-clean/020/008/020-008-017.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(train_set)))\n",
    "train_set = torch.utils.data.DataLoader(masese_train, batch_size=64, shuffle=True,\n",
    "                                        num_workers=2, collate_fn=collate_fn_libri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_libri(batch):\n",
    "    #print(batch)\n",
    "    tensors = [b[0].t() for b in batch if b]\n",
    "    tensors_len = [len(t) for t in tensors]\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "    \n",
    "    transcriptions = [list(b[2].replace(\"'\", \" \")) for b in batch if b]\n",
    "    targets = [torch.tensor([tokens_list.index(e) for e in t]) for t in transcriptions]\n",
    "    targets_len = [len(t) for t in targets]\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "    return tensors, targets, torch.tensor(tensors_len), torch.tensor(targets_len)\n",
    "\n",
    "# valid_set = torch.utils.data.DataLoader(masese_dev, batch_size=1500, shuffle=True,\n",
    "#                                         num_workers=4, collate_fn=collate_fn_libri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(valid_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/facebookresearch/CPC_audio.git  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/AMMI-Speech-Lig-Aikuma/Dataset/Project/CPC_audio\n"
     ]
    }
   ],
   "source": [
    "%cd CPC_audio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !%cd /CPC_audio\n",
    "# !python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Building the model\n",
    "\n",
    "In this exercise, we will build and train a small CPC model using the repository CPC_audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/CPC_audio\n",
    "from cpc.model import CPCEncoder, CPCAR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DIM_ENCODER= 256 #256\n",
    "DIM_CONTEXT= 256 #256\n",
    "KEEP_HIDDEN_VECTOR=False\n",
    "N_LEVELS_CONTEXT=1\n",
    "CONTEXT_RNN=\"LSTM\"\n",
    "N_PREDICTIONS=12\n",
    "LEARNING_RATE=2e-4\n",
    "N_NEGATIVE_SAMPLE =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CPCEncoder(DIM_ENCODER)\n",
    "context = CPCAR(DIM_ENCODER,\n",
    "                DIM_CONTEXT,\n",
    "                KEEP_HIDDEN_VECTOR,\n",
    "                N_LEVELS_CONTEXT,\n",
    "                mode=\"CONTEXT_RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several functions that will be necessary to load the data later\n",
    "from cpc.dataset import findAllSeqs, AudioBatchData, parseSeqLabels\n",
    "SIZE_WINDOW = 20480\n",
    "BATCH_SIZE=8\n",
    "def load_dataset(path_dataset, file_extension='.wav', phone_label_dict=None):\n",
    "    data_list, speakers = findAllSeqs(path_dataset, extension=file_extension)\n",
    "    dataset = AudioBatchData(path_dataset, SIZE_WINDOW, data_list, phone_label_dict, len(speakers))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPCModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 AR):\n",
    "\n",
    "        super(CPCModel, self).__init__()\n",
    "        self.gEncoder = encoder\n",
    "        self.gAR = AR\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        \n",
    "\n",
    "        encoder_output = self.gEncoder(batch_data)\n",
    "        #print(encoder_output.shape)\n",
    "        # The output of the encoder data does not have the good format \n",
    "        # indeed it is Batch_size x Hidden_size x temp size\n",
    "        # while the context requires Batch_size  x temp size x Hidden_size\n",
    "        # thus you need to permute\n",
    "        context_input = encoder_output.permute(0, 2, 1)\n",
    "\n",
    "        context_output = self.gAR(context_input)\n",
    "        #print(context_output.shape)\n",
    "        return context_output, encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torchaudio.load(\n",
    "#         \"../voalingala/20200611/20200611-160000-VCD361-program_16k.mp3\")[0]\n",
    "        \"../MaseseSpeech/train-clean/020/001/020-001-013.wav\")[0]\n",
    "        \n",
    "\n",
    "audio = audio.view(1, 1, -1)\n",
    "\n",
    "cpc_model = CPCModel(encoder, context).to(device)\n",
    "context_output, encoder_output = cpc_model(audio.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.1118, 0.0728, 0.0479]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cpc.dataset import parseSeqLabels\n",
    "# from cpc.feature_loader import loadModel\n",
    "\n",
    "# checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "# cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "# cpc_model = cpc_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 : CPC loss\n",
    "\n",
    "We will define a class ```CPCCriterion``` which will hold the prediction networks $\\phi_k$ defined above and perform the classification loss $\\mathcal{L}_c$.\n",
    "\n",
    "a) In this exercise, the $\\phi_k$ will be a linear transform, ie:\n",
    "\n",
    "\\\\[ \\phi_k(c_t) = \\mathbf{A}_k c_t\\\\]\n",
    "\n",
    "Using the class [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear), define the transformations $\\phi_k$ in the code below and complete the function ```get_prediction_k``` which computes $\\phi_k(c_t)$ for a given batch of vectors $c_t$.\n",
    "\n",
    "b) Using both ```get_prediction_k```  and ```sample_negatives``` defined below, write the forward function which will take as input two batches of features $c_t$ and $g_t$ and outputs the classification loss $\\mathcal{L}_c$ and the average acuracy for all predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2: write the CPC loss\n",
    "# a) Write the negative sampling (with some help)\n",
    "# ERRATUM: it's really hard, the sampling will be provided\n",
    "\n",
    "class CPCCriterion(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 K,\n",
    "                 dim_context,\n",
    "                 dim_encoder,\n",
    "                 n_negative):\n",
    "        \n",
    "        super(CPCCriterion, self).__init__()\n",
    "        \n",
    "        self.K_ = K\n",
    "        self.dim_context = dim_context\n",
    "        self.dim_encoder = dim_encoder\n",
    "        self.n_negative = n_negative\n",
    "\n",
    "        self.predictors = torch.nn.ModuleList()\n",
    "        \n",
    "        for k in range(self.K_):\n",
    "            # TO COMPLETE !\n",
    "            # A affine transformation in pytorch is equivalent to a nn.Linear layer\n",
    "            # To get a linear transformation you must set bias=False\n",
    "            # input dimension of the layer = dimension of the encoder\n",
    "            # output dimension of the layer = dimension of the context\n",
    "            self.predictors.append(torch.nn.Linear(dim_context, dim_encoder, bias=False))\n",
    "        \n",
    "    def get_prediction_k(self, context_data):\n",
    "        #TO COMPLETE !\n",
    "        output = [] \n",
    "        # For each time step k\n",
    "        for k in range(self.K_):\n",
    "            # We need to compute phi_k = A_k * c_t\n",
    "            phi_k = self.predictors[k](context_data)\n",
    "            output.append(phi_k)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def sample_negatives(self, encoded_data):\n",
    "        r\"\"\"\n",
    "        Sample some negative examples in the given encoded data.\n",
    "        Input:\n",
    "        - encoded_data size: B x T x H\n",
    "        Returns\n",
    "        - outputs of size B x (n_negative + 1) x (T - K_) x H\n",
    "          outputs[:, 0, :, :] contains the positive example\n",
    "          outputs[:, 1:, :, :] contains negative example sampled in the batch\n",
    "        - labels, long tensor of size B x (T - K_)\n",
    "          Since the positive example is always at coordinates 0 for all sequences \n",
    "          in the batch and all timestep in the sequence, labels is just a tensor\n",
    "          full of zeros !\n",
    "        \"\"\"\n",
    "        batch_size, time_size, dim_encoded = encoded_data.size()\n",
    "        window_size = time_size - self.K_\n",
    "        outputs = []\n",
    "\n",
    "        neg_ext = encoded_data.contiguous().view(-1, dim_encoded)\n",
    "        n_elem_sampled = self.n_negative * window_size * batch_size\n",
    "        # Draw nNegativeExt * batchSize negative samples anywhere in the batch\n",
    "        batch_idx = torch.randint(low=0, high=batch_size,\n",
    "                                  size=(n_elem_sampled, ),\n",
    "                                  device=encoded_data.device)\n",
    "\n",
    "        seq_idx = torch.randint(low=1, high=time_size,\n",
    "                                size=(n_elem_sampled, ),\n",
    "                                device=encoded_data.device)\n",
    "\n",
    "        base_idx = torch.arange(0, window_size, device=encoded_data.device)\n",
    "        base_idx = base_idx.view(1, 1, window_size)\n",
    "        base_idx = base_idx.expand(1, self.n_negative, window_size)\n",
    "        base_idx = base_idx.expand(batch_size, self.n_negative, window_size)\n",
    "        seq_idx += base_idx.contiguous().view(-1)\n",
    "        seq_idx = torch.remainder(seq_idx, time_size)\n",
    "\n",
    "        ext_idx = seq_idx + batch_idx * time_size\n",
    "        neg_ext = neg_ext[ext_idx].view(batch_size, self.n_negative,\n",
    "                                        window_size, dim_encoded)\n",
    "        label_loss = torch.zeros((batch_size, window_size),\n",
    "                                  dtype=torch.long,\n",
    "                                  device=encoded_data.device)\n",
    "\n",
    "        for k in range(1, self.K_ + 1):\n",
    "            # Positive samples\n",
    "            if k < self.K_:\n",
    "                pos_seq = encoded_data[:, k:-(self.K_-k)]\n",
    "            else:\n",
    "                pos_seq = encoded_data[:, k:]\n",
    "            pos_seq = pos_seq.view(batch_size, 1, pos_seq.size(1), dim_encoded)\n",
    "            full_seq = torch.cat((pos_seq, neg_ext), dim=1)\n",
    "            outputs.append(full_seq)\n",
    "\n",
    "        return outputs, label_loss\n",
    "    \n",
    "    def forward(self, encoded_data, context_data):\n",
    "\n",
    "        # TO COMPLETE:\n",
    "        # Perform the full cpc criterion\n",
    "        # Returns 2 values:\n",
    "        # - the average classification loss avg_loss\n",
    "        # - the average classification acuracy avg_acc\n",
    "\n",
    "        # Reminder : The permuation !\n",
    "        encoded_data = encoded_data.permute(0, 2, 1)\n",
    "\n",
    "        # First we need to sample the negative examples\n",
    "        negative_samples, labels = self.sample_negatives(encoded_data)\n",
    "\n",
    "        # Then we must compute phi_k\n",
    "        phi_k = self.get_prediction_k(context_data)\n",
    "\n",
    "        # Finally we must get the dot product between phi_k and negative_samples \n",
    "        # for each k\n",
    "\n",
    "        #The total loss is the average of all losses\n",
    "        avg_loss = 0\n",
    "\n",
    "        # Average acuracy\n",
    "        avg_acc = 0\n",
    "\n",
    "        for k in range(self.K_):\n",
    "            B, N_sampled, S_small, H = negative_samples[k].size() \n",
    "            B, S, H = phi_k[k].size()\n",
    "            \n",
    "            # As told before S = S_small + K. For segments too far in the sequence\n",
    "            # there are no positive exmples anyway, so we must shorten phi_k\n",
    "            phi = phi_k[k][:, :S_small]\n",
    "            \n",
    "            # Now the dot product\n",
    "            # You have several ways to do that, let's do the simple but non optimal \n",
    "            # one\n",
    "            # pytorch has a matrix product function https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "            # But it takes only 3D tensors of the same batch size !\n",
    "            # To begin negative_samples is a 4D tensor ! \n",
    "            # We want to compute the dot product for each features, of each sequence\n",
    "            # of the batch. Thus we are trying to compute a dot product for all\n",
    "            # B* N_sampled * S_small 1D vector of negative_samples[k]\n",
    "            # Or, a 1D tensor of size H is also a matrix of size 1 x H\n",
    "            # Then, we must view it as a 3D tensor of size (B* N_sampled * S_small, 1, H)\n",
    "            negative_sample_k  =  negative_samples[k].view(B* N_sampled* S_small, 1, H)\n",
    "            \n",
    "            # But now phi and negative_sample_k no longer have the same batch size !\n",
    "            # No worries, we can expand phi so that each sequence of the batch\n",
    "            # is repeated N_sampled times\n",
    "            phi = phi.view(B, 1,S_small, H).expand(B, N_sampled, S_small, H)\n",
    "            \n",
    "            # And now we can view it as a 3D tensor \n",
    "            phi  = phi.contiguous().view(B * N_sampled * S_small, H, 1)\n",
    "            \n",
    "            # We can finally get the dot product !\n",
    "            scores = torch.bmm(negative_sample_k, phi)\n",
    "            \n",
    "            # Dot_product has a size (B * N_sampled * S_small , 1, 1)\n",
    "            # Let's reorder it a bit\n",
    "            scores = scores.reshape(B, N_sampled, S_small)\n",
    "            \n",
    "            # For each elements of the sequence, and each elements sampled, it gives \n",
    "            # a floating score stating the likelihood of this element being the \n",
    "            # true one.\n",
    "            # Now the classification loss, we need to use the Cross Entropy loss\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html\n",
    "            \n",
    "            # For each time-step of each sequence of the batch \n",
    "            # we have N_sampled possible predictions. \n",
    "            # Looking at the documentation of torch.nn.CrossEntropyLoss\n",
    "            # we can see that this loss expect a tensor of size M x C where \n",
    "            # - M is the number of elements with a classification score\n",
    "            # - C is the number of possible classes\n",
    "            # There are N_sampled candidates for each predictions so\n",
    "            # C = N_sampled \n",
    "            # Each timestep of each sequence of the batch has a prediction so\n",
    "            # M = B * S_small\n",
    "            # Thus we need an input vector of size B * S_small, N_sampled\n",
    "            # To begin, we need to permute the axis\n",
    "            scores = scores.permute(0, 2, 1) # Now it has size B , S_small, N_sampled\n",
    "            \n",
    "            # Then we can cast it into a 2D tensor\n",
    "            scores = scores.reshape(B * S_small, N_sampled)\n",
    "            \n",
    "            # Same thing for the labels \n",
    "            labels = labels.reshape(B * S_small)\n",
    "            \n",
    "            # Finally we can get the classification loss\n",
    "            loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "            loss_k = loss_criterion(scores, labels)\n",
    "            avg_loss+= loss_k\n",
    "            \n",
    "            # And for the acuracy\n",
    "            # The prediction for each elements is the sample with the highest score\n",
    "            # Thus the tensors of all predictions is the tensors of the index of the \n",
    "            # maximal score for each time-step of each sequence of the batch\n",
    "            predictions = torch.argmax(scores, 1)\n",
    "            acc_k  = (labels == predictions).sum() / (B * S_small)\n",
    "            avg_acc += acc_k\n",
    "\n",
    "        # Normalization\n",
    "        avg_loss = avg_loss / self.K_\n",
    "        avg_acc = avg_acc / self.K_\n",
    "        \n",
    "        return avg_loss , avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "audio = torchaudio.load(\n",
    "    \"../MaseseSpeech/train-clean/020/001/020-001-013.wav\")[0]\n",
    "#     \"../voalingala/20200611/20200611-160000-VCD361-program_16k.mp3\")[0]\n",
    "\n",
    "audio = audio.view(1, 1, -1)\n",
    "cpc_criterion = CPCCriterion(N_PREDICTIONS, DIM_CONTEXT, \n",
    "                             DIM_ENCODER, N_NEGATIVE_SAMPLE).to(device)\n",
    "context_output, encoder_output = cpc_model(audio.to(device))\n",
    "loss, avg = cpc_criterion(encoder_output,context_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3424, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Full training loop !\n",
    "\n",
    "You have the model, you have the criterion. All you need now are a data loader and an optimizer to run your training loop.\n",
    "\n",
    "We will use an Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(cpc_criterion.parameters()) + list(cpc_model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 5955.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache file at ../MaseseSpeech/train-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "363it [00:00, 855355.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking length...\n",
      "Done, elapsed: 0.029 seconds\n",
      "Scanned 363 sequences in 0.03 seconds\n",
      "1 chunks computed\n",
      "Joining pool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset_train = load_dataset('../voalingala/train',\n",
    "#                              file_extension='.mp3')\n",
    "# dataset_val = load_dataset('../voalingala/val',\n",
    "#                              file_extension='.mp3')\n",
    "\n",
    "dataset_train = load_dataset('../MaseseSpeech/train-clean')\n",
    "dataset_val = load_dataset('../MaseseSpeech/dev-clean')\n",
    "\n",
    "data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n",
    "data_loader_val = dataset_train.getDataLoader(BATCH_SIZE, \"sequence\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
